<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/cc.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/cc.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/cc.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="虫虫呀">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="虫虫呀">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="虫虫呀">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '谢江琼'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>虫虫呀</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">虫虫呀</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">苦练含笑半步癫</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/24/在redis中使用的keys命令引起的灾难/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="xiejq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/WechatIMG2.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="虫虫呀">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/24/在redis中使用的keys命令引起的灾难/" itemprop="url">在redis中使用的keys命令引起的灾难</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-09-24T11:29:46+08:00">
                2018-09-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="在redis中使用的keys命令引起的灾难"><a href="#在redis中使用的keys命令引起的灾难" class="headerlink" title="在redis中使用的keys命令引起的灾难"></a>在redis中使用的keys命令引起的灾难</h2><blockquote>
<p>一直在书上看到redis中不能使用keys命令,9.20号周五下午四点多,访问应用变慢,之后运维组发现是由于redis的keys命令导致了缓存挂了。</p>
</blockquote>
<h3 id="分析keys命令如何使redis阻塞"><a href="#分析keys命令如何使redis阻塞" class="headerlink" title="分析keys命令如何使redis阻塞"></a>分析keys命令如何使redis阻塞</h3><p>  Redis的使用为了保证高可用性,一般开启了主从+keepalived,用虚IP地址在master和slave两遍漂移,master挂了直接切换到slave。这种方案很好但是忽略了主数据节点挂掉的情况。Redis是单进程、单线程设计是其简单和稳定的继承，只要不是服务器发生了故障，在一般情况下是不会挂的。但同时，单进程、单线程的设计会导致redis接收到复杂指令时会忙于计算而停止响应，可能就因为一个Zset或者keys之类的指令,redis计算时间稍长，Keepalived就认为其停止了响应，直接更改虚IP的指向,然后做一次主从切换。过不了多久，zset和keys之类的指令又会从客户端发送过来，于是从机上又开始阻塞，Keepalived就一直在主从机之间不停地切换IP。终于主节点和从节点都堵了,Keepalived发现后，居然直接把虚IP释放了，然后所有的客户端都无法连接redis了，只能等运维到线上手工绑定才行。</p>
<h3 id="使用redis注意点"><a href="#使用redis注意点" class="headerlink" title="使用redis注意点"></a>使用redis注意点</h3><ol>
<li>redis被keys命令阻塞了</li>
<li>Keepalived切换虚IP失败,虚IP被释放了</li>
<li>用Redis做计算了,Redis的CPU占用率成了100%了</li>
<li>主从同步失败了</li>
<li>Redis客户端连接数爆了(网卡打满)</li>
</ol>
<p><img src="/2018/09/24/在redis中使用的keys命令引起的灾难/Users/cc/limedroid.github.io/source/_posts/在redis中使用的keys命令引起的灾难/redids.png" alt="redids"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol>
<li>客户端Jedis不能有keys命令的接口防止开发人员误用</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/28/java数据类型和自动装箱/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="xiejq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/WechatIMG2.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="虫虫呀">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/28/java数据类型和自动装箱/" itemprop="url">java数据类型和自动装箱</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-28T23:30:33+08:00">
                2018-01-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="java数据类型和boxing"><a href="#java数据类型和boxing" class="headerlink" title="java数据类型和boxing"></a>java数据类型和boxing</h2><h3 id="一段代码的执行结果"><a href="#一段代码的执行结果" class="headerlink" title="一段代码的执行结果"></a>一段代码的执行结果</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Integer a = <span class="number">1</span>;</span><br><span class="line">Integer b = <span class="number">2</span>;</span><br><span class="line">Integer c = <span class="number">3</span>;</span><br><span class="line">Integer d = <span class="number">3</span>;</span><br><span class="line">Integer e = <span class="number">321</span>;</span><br><span class="line">Integer f = <span class="number">321</span>;</span><br><span class="line">Long g = <span class="number">3L</span>;</span><br><span class="line">System.out.println(c == d); <span class="comment">// true</span></span><br><span class="line">System.out.println(e == f); <span class="comment">// false</span></span><br><span class="line">System.out.println(c == a+b); <span class="comment">// true</span></span><br><span class="line">System.out.println(c.equals(a+b)); <span class="comment">//true</span></span><br><span class="line">System.out.println(g == a+b); <span class="comment">//true</span></span><br><span class="line">System.out.println(g.equals(a+b)); <span class="comment">//false</span></span><br></pre></td></tr></table></figure>
<h3 id="自动装箱的过程"><a href="#自动装箱的过程" class="headerlink" title="自动装箱的过程"></a>自动装箱的过程</h3><p><strong>自动装箱的过程会有缓存的过程，默认缓存-128~127的整型</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Returns an &#123;<span class="doctag">@code</span> Integer&#125; instance representing the specified</span></span><br><span class="line"><span class="comment"> * &#123;<span class="doctag">@code</span> int&#125; value.  If a new &#123;<span class="doctag">@code</span> Integer&#125; instance is not</span></span><br><span class="line"><span class="comment"> * required, this method should generally be used in preference to</span></span><br><span class="line"><span class="comment"> * the constructor &#123;<span class="doctag">@link</span> #Integer(int)&#125;, as this method is likely</span></span><br><span class="line"><span class="comment"> * to yield significantly better space and time performance by</span></span><br><span class="line"><span class="comment"> * caching frequently requested values.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * This method will always cache values in the range -128 to 127,</span></span><br><span class="line"><span class="comment"> * inclusive, and may cache other values outside of this range.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span>  i an &#123;<span class="doctag">@code</span> int&#125; value.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> an &#123;<span class="doctag">@code</span> Integer&#125; instance representing &#123;<span class="doctag">@code</span> i&#125;.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span>  1.5</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Integer <span class="title">valueOf</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">assert</span> IntegerCache.high &gt;= <span class="number">127</span>;</span><br><span class="line">    <span class="keyword">if</span> (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high)</span><br><span class="line">        <span class="keyword">return</span> IntegerCache.cache[i + (-IntegerCache.low)];</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Integer(i);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="unboxing"><a href="#unboxing" class="headerlink" title="unboxing"></a>unboxing</h3><p>自动拆箱:自动取出打包器的基本形态信息</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Integer a = <span class="number">1</span>;</span><br><span class="line">Integer b = <span class="number">2</span>;</span><br><span class="line">Long g = <span class="number">3L</span>;</span><br><span class="line">System.out.println(g == a+b); <span class="comment">//true</span></span><br><span class="line">System.out.println(g.equals(a+b)); <span class="comment">//false</span></span><br></pre></td></tr></table></figure>
<p>在进行运算的时候，编译程序先对a和b自动拆箱，再进行加法运算。</p>
<p>== 比较时对数据类型进行提升。左右两边都是long类型</p>
<p>equals比较时由于数据类型不同为false。</p>
<p><a href="https://www.cnblogs.com/lwbqqyumidi/p/3700164.html" target="_blank" rel="noopener">java数据类型</a></p>
<blockquote>
<p>自动装箱调用了Integer.valueOf()</p>
<p>自动拆箱用了Interger.intValue()</p>
<p>鉴于包装类的==运算在不遇到算数运算符的情况下不会自动拆箱，以及它们的equals()方法不处理数据转型的关系，深入理解JVM的作用建议在实际编码中不使用自动装箱和拆箱。</p>
</blockquote>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/15/abstract-method和interface的区别/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="xiejq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/WechatIMG2.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="虫虫呀">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/15/abstract-method和interface的区别/" itemprop="url">abstract method和interface的区别</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-15T16:29:26+08:00">
                2018-01-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="abstract-method和interface的区别"><a href="#abstract-method和interface的区别" class="headerlink" title="abstract method和interface的区别"></a>abstract method和interface的区别</h2><h3 id="语法层面上的区别"><a href="#语法层面上的区别" class="headerlink" title="语法层面上的区别"></a>语法层面上的区别</h3><ol>
<li>抽象类可以提供成员方法的细节，而接口中只能存在public abstract方法</li>
<li>抽象类中的成员变量可以是各种类型的，而接口中只能是public static final类型的</li>
<li>接口中不能含有静态代码块以及静态方法</li>
</ol>
<hr>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//接口中的方法默认为public abstract类型的,在接口中不用public修饰方法，IDE反而提示多余</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//由于接口中不能含有代码块，也就是不能定义构造函数，因此接口中的属性必须直接初始化</span></span><br><span class="line"><span class="comment">//静态类型虽然可以在定义的时候不初始化，在构造函数中初始化，但是在接口中必须直接初始化</span></span><br><span class="line">String name = <span class="string">"xiejq"</span>;<span class="comment">//没有显式说明是public static类型的</span></span><br></pre></td></tr></table></figure>
<h3 id="设计层面上的区别"><a href="#设计层面上的区别" class="headerlink" title="设计层面上的区别"></a>设计层面上的区别</h3><p>  abstract method通常作为设计模式<strong>模板设计模式</strong>的最顶端上的类，用来封装算法。</p>
<p>比如处理订单过程中，有订单对象生成(<code>init</code>)预处理这些操作，订单信息入库这些公共的操作，在Abstract method中存在具体实现，在Abstract method中不具体实现preProcess()，process(),afterProcess()的过程，通过不同业务的对象来继承这个抽象对象，生成订单的时候执行自己的预处理 处理 处理完成的代码，在设计模式上是模板设计模式。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Order <span class="title">newOrder</span><span class="params">(NewBusinessParams businessParams)</span> <span class="keyword">throws</span> EcpsException </span>&#123;</span><br><span class="line">	<span class="keyword">this</span>.businessParams = businessParams;</span><br><span class="line">	init();</span><br><span class="line">	validate();</span><br><span class="line">	<span class="keyword">try</span> &#123;</span><br><span class="line">		preProcess();	</span><br><span class="line">		process();</span><br><span class="line">		afterProcess();	</span><br><span class="line">	&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">		<span class="keyword">try</span> &#123;</span><br><span class="line">			finalProcess();</span><br><span class="line">		&#125; <span class="keyword">catch</span>(Exception e) &#123;</span><br><span class="line">			logger.error(<span class="string">"处理finalProcess失败"</span>, e);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> order;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>  接口的辐射设计模式，如果这个对象有接口中含有的行为，就实现这个接口中的抽象方法。</p>
<p><a href="http://www.imooc.com/article/5093" target="_blank" rel="noopener">参考了这篇文章以及我们的工程</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/13/迭代器和可迭代对象/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="xiejq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/WechatIMG2.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="虫虫呀">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/13/迭代器和可迭代对象/" itemprop="url">迭代器和可迭代对象</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-13T11:21:46+08:00">
                2018-01-13
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="iterable和iterator"><a href="#iterable和iterator" class="headerlink" title="iterable和iterator"></a>iterable和iterator</h2><p>在python和java中都有iterable和iterator，两种语言对着两个对象的使用规则是一样的。</p>
<blockquote>
<p>看了一篇博文<a href="http://182.61.41.220:8080/blog/article.ftl?id=1" target="_blank" rel="noopener">java集合的问题</a>，在java中删除元素，用集合中删除元素中循环删除多个元素不能用collection中的remove方法，而应该用iterator中的remove方法。</p>
</blockquote>
<p>分析下为什么。</p>
<h3 id="增强式for循环是编译的蜜糖"><a href="#增强式for循环是编译的蜜糖" class="headerlink" title="增强式for循环是编译的蜜糖"></a>增强式for循环是编译的蜜糖</h3><p>增强式for循环可以用在数组等等上面</p>
<p>增强式for循环用在操作Iterable接口的对象上面</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">forEach</span><span class="params">(Iterable iterable)</span></span>&#123;</span><br><span class="line">  <span class="keyword">for</span>(Object o : iterable)&#123;</span><br><span class="line">    <span class="comment">//执行的代码块</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//会在编译程序展开为</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">forEach</span><span class="params">(Iterable iterable)</span></span>&#123;</span><br><span class="line">  Object o;</span><br><span class="line">  <span class="keyword">for</span>(Iterator i$ = iterable.iterator();i$.hasNext();代码块)&#123;</span><br><span class="line">    o = i$.next()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//可以看到增强式for循环实际上是用了iterator对象的实例。</span></span><br></pre></td></tr></table></figure>
<p>文章第一种方式移除元素可以看出，集合自身的remove操作会修改自身含有的数据，实例自身的结构会变</p>
<p>文章的第二种移除元素的方式是：在增强式for循环中执行collection里面定义的remove方法。发生的</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread <span class="string">"main"</span> java.util.ConcurrentModificationException</span><br><span class="line">	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:<span class="number">901</span>)</span><br><span class="line">	at java.util.ArrayList$Itr.next(ArrayList.java:<span class="number">851</span>)</span><br><span class="line">	at org.ai.cc.examples.App.main(App.java:<span class="number">42</span>)</span><br><span class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:<span class="number">62</span>)</span><br><span class="line">	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:<span class="number">43</span>)</span><br><span class="line">	at java.lang.reflect.Method.invoke(Method.java:<span class="number">497</span>)</span><br><span class="line">	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:<span class="number">147</span>)</span><br></pre></td></tr></table></figure>
<p><strong>修正原文，不一定要是最后一个元素，在增强式for循环中执行集合本身的remove方法都会报这个错</strong></p>
<p>原因：</p>
<blockquote>
<p>iterator在调用next方法时，会检查列表是否被修改过，如果被修改过，抛出ConcurrentModificationException异常。也就是在Collection或Map在迭代过程中尝试直接修改Collection或Map的内容时会抛异常</p>
<p>Iterator被创建后会建立一个指向原来对象的单链索引，当原来的对象数量发生变化时，这个索引表的内容不会同步改变，所以当索引针织往后移动就找不到要迭代的对象，按照fail-fast原则Iterator会马上抛出java.util.ConcurrentModificationException</p>
<p>但是使用iterator本身方法remove()对象时，Iterator.remove()方法会在删除当前迭代对象的同时维护索引的一致性</p>
</blockquote>
<p>总结：</p>
<ol>
<li>增强式for循环实际是内部类产生的迭代器iterator,操作了iterator的hasNext()和next()，因为next()方法检查到有变化就会抛出异常，因此在增强式for循环不能直接修改C和Map的内容</li>
<li>在Map,Collection,不要用它们的API直接修改集合的内容，用iterator的remove()方法</li>
</ol>
<ol>
<li>jdk1.5以前iterator方法在Collection中，jdk1.5~1.7增加了Iterable接口，Collection实现了Iterable接口，Iterable接口里面只有iteartor()方法，只要实现了Iterable接口，就能使用for(xx:xx)语句</li>
<li>jdk1.8给Iterable接口增加了更多的方法。待学习。java的Iterable和iterator是一模一样的机制。</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/08/cifar数据集/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="xiejq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/WechatIMG2.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="虫虫呀">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/08/cifar数据集/" itemprop="url">cifar数据集</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-08T17:08:05+08:00">
                2018-01-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="cifar数据集官方demo"><a href="#cifar数据集官方demo" class="headerlink" title="cifar数据集官方demo"></a>cifar数据集官方demo</h2><p><img src="/2018/01/08/cifar数据集/professional_demo.png" alt="professional_demo"></p>
<h3 id="数据集的结构"><a href="#数据集的结构" class="headerlink" title="数据集的结构"></a>数据集的结构</h3><h4 id="数据集下载和解压"><a href="#数据集下载和解压" class="headerlink" title="数据集下载和解压"></a>数据集下载和解压</h4><p>源码是cifar.py</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/08/检验softmax/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="xiejq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/WechatIMG2.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="虫虫呀">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/08/检验softmax/" itemprop="url">检验softmax</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-08T09:05:47+08:00">
                2018-01-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="是否大于5岁的测试demo"><a href="#是否大于5岁的测试demo" class="headerlink" title="是否大于5岁的测试demo"></a>是否大于5岁的测试demo</h2><p><a href="http://geek.csdn.net/news/detail/163735" target="_blank" rel="noopener">csdn</a></p>
<p><a href="https://github.com/ccwhale/tensorflowDemo/blob/master/trials/validateSoftmax.py" target="_blank" rel="noopener">可执行代码的位置</a></p>
<p><img src="/2018/01/08/检验softmax/firstpicture.png" alt="firstpicture"></p>
<h3 id="生成训练数据和测试数据"><a href="#生成训练数据和测试数据" class="headerlink" title="生成训练数据和测试数据"></a>生成训练数据和测试数据</h3><p>模型的label数据集是one-hot编码或者是它的真实值</p>
<p>如果year大于5岁，则标签设置为：[0, 0, 1]；  数组的第二个位置为1，因此标签的真实分类是2</p>
<p>否则，标签设置为：[0, 1, 0]。数组的第一个位置为1，因此标签的真实分类是1</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成(6000,2)的训练集合</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'label.txt'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> file:</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'train.txt'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">6000</span>):</span><br><span class="line">            a = random.randint(<span class="number">0</span>, <span class="number">9</span>)</span><br><span class="line">            <span class="keyword">if</span> a &gt; <span class="number">5</span>:</span><br><span class="line">                file.write(<span class="string">'2'</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                file.write(<span class="string">'1'</span>)</span><br><span class="line">            f.write(str(a) + <span class="string">'1'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成(1000,2)的训练集合</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'label_test.txt'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> file:</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'train_test.txt'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">            a = random.randint(<span class="number">0</span>, <span class="number">9</span>)</span><br><span class="line">            <span class="keyword">if</span> a &gt; <span class="number">5</span>:</span><br><span class="line">                file.write(<span class="string">'2'</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                file.write(<span class="string">'1'</span>)</span><br><span class="line">            f.write(str(a) + <span class="string">'1'</span>)</span><br></pre></td></tr></table></figure>
<h2 id="定义训练模型"><a href="#定义训练模型" class="headerlink" title="定义训练模型"></a>定义训练模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义训练模型 tf.placeholder是是需要喂养的数据 tf.Variable()是需要通过训练不断优化得到的变量</span></span><br><span class="line"><span class="comment"># y_是[None,2]训练数据的真实分类</span></span><br><span class="line">    x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">2</span>])</span><br><span class="line">    w = tf.Variable(tf.zeros([<span class="number">2</span>, <span class="number">3</span>]))</span><br><span class="line">    b = tf.Variable(tf.zeros([<span class="number">3</span>]))</span><br><span class="line">    y = tf.matmul(x, w) + b</span><br><span class="line">    y_ = tf.placeholder(tf.int64, [<span class="keyword">None</span>])  <span class="comment"># 一维的,就是它的真实值</span></span><br></pre></td></tr></table></figure>
<p><img src="/2018/01/08/检验softmax/linear_model.jpg" alt="linear_model"></p>
<p>一次输入就会更新定义的6个w和3个b(bias偏差)</p>
<h2 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cross_entropy = tf.losses.sparse_softmax_cross_entropy(logits=y, labels=y_)</span><br></pre></td></tr></table></figure>
<p>logits=y 是预测值</p>
<p>labels=y_ 是真实值</p>
<p><a href="https://ccwhale.github.io/2017/12/28/tensorflowAPI-and-Mathematics/" target="_blank" rel="noopener">以reduce开头的api将二维矩阵压扁</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这个函数是由一系列操作封装好的</span></span><br><span class="line"><span class="comment"># The raw formulation of cross-entropy,</span></span><br><span class="line"><span class="comment">#  python有一组以reduce开头的api,作用是给将二维矩阵压扁。0代表行1代表列，</span></span><br><span class="line"><span class="comment">#  因此下面代码是按列将数组压扁，按照列，就是从列看过去，从左边看过去，压缩二维数组，也就是取每行的和</span></span><br><span class="line"><span class="comment">#  如果没有指定行或者列，取所有值的平均值</span></span><br><span class="line"><span class="comment">#   tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.nn.softmax(y)), # 矩阵的dot product</span></span><br><span class="line"><span class="comment">#                                        reduction_indices=[1]))</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># can be numerically unstable.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># So here we use tf.losses.sparse_softmax_cross_entropy on the raw</span></span><br><span class="line"><span class="comment"># logit outputs of 'y', and then average across the batch.</span></span><br></pre></td></tr></table></figure>
<h2 id="定义优化函数"><a href="#定义优化函数" class="headerlink" title="定义优化函数"></a>定义优化函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(cross_entropy)</span><br></pre></td></tr></table></figure>
<p>tensorflow中包含很多优化函数。优化函数有两个参数:</p>
<ol>
<li>learning rate 学习率，决定每次输入一次数据，根据偏导更新每个权重的步长</li>
<li>cross_entropy 交叉熵函数，学习的过程就是使函数的熵值变小。</li>
</ol>
<p><strong>优化函数的输入也是函数</strong></p>
<p><a href="https://ccwhale.github.io/2018/01/05/MNIST-FOR-ML/" target="_blank" rel="noopener">交叉熵为什么可以评价权重的好坏</a></p>
<p>0.5是优化函数的learning rate。</p>
<h3 id="梯度下降-Gradient-Descent-的策略"><a href="#梯度下降-Gradient-Descent-的策略" class="headerlink" title="梯度下降(Gradient Descent)的策略"></a>梯度下降(Gradient Descent)的策略</h3><h4 id="使用可以自我调整learning-rate的优化函数"><a href="#使用可以自我调整learning-rate的优化函数" class="headerlink" title="使用可以自我调整learning rate的优化函数"></a>使用可以自我调整learning rate的优化函数</h4><p>在训练的过程中，我们假定刚开始训练的时候，离cross_entropy的最小值(最低点)很远，这个时候我们希望learning rate很大。当训练快要接近函数的最低点的时候，我们希望learning rate很小。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.8</span>).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    调整不同的优化函数显示不同的准确度(控制learning Rate为0.5)</span></span><br><span class="line"><span class="string">    ==============================</span></span><br><span class="line"><span class="string">    | AdamOptimizer      | 0.8679</span></span><br><span class="line"><span class="string">    ==============================</span></span><br><span class="line"><span class="string">    | AdagradOptimizer   | 0.9202</span></span><br><span class="line"><span class="string">    ==============================</span></span><br><span class="line"><span class="string">    | GradientDescentOptimizer | 0.9152</span></span><br><span class="line"><span class="string">    ==============================</span></span><br><span class="line"><span class="string">    调整不同的learning rate(控制梯度函数为GradientDescentOptimizer 0.5=0.9152 )</span></span><br><span class="line"><span class="string">    ==============================</span></span><br><span class="line"><span class="string">    | 0.1                | 0.9097</span></span><br><span class="line"><span class="string">    ==============================</span></span><br><span class="line"><span class="string">    | 0.2                | 0.9163</span></span><br><span class="line"><span class="string">    ==============================</span></span><br><span class="line"><span class="string">    | 0.3                 | 0.9181</span></span><br><span class="line"><span class="string">    ==============================</span></span><br><span class="line"><span class="string">    | 0.8                 | 0.9234</span></span><br><span class="line"><span class="string">    ==============================</span></span><br></pre></td></tr></table></figure>
<h5 id="AdagradOptimizer"><a href="#AdagradOptimizer" class="headerlink" title="AdagradOptimizer"></a>AdagradOptimizer</h5><p>是一个自我调整learning rate的函数</p>
<p>$$w^{t+1} = w^t - \frac{\eta}{\sum_{i=0}^t (g^i)^2} * g^t$$</p>
<p>优化函数认为训练的开始梯度Gradient(也就是偏导)很大，离训练的终点很小，需要很大的learning rate</p>
<p>在AdagradOptimizer方法中：</p>
<p>$g^t$很大的时候，分母也很大，$\eta$就很小</p>
<p>$g^t$很小的时候，分母不一定很小，因为分母是前面所有导数的和，$\eta$就可能还是很小</p>
<p><strong>AdagradOptimizer</strong> 的作用是强化反差：</p>
<p>当$g^t$之前都很小，突然变得很大，$\eta$就变的很小，此时移动的步长就很小了(不知道理解的对不对，待修正)</p>
<h5 id="Vanilla-Gradient-Descent"><a href="#Vanilla-Gradient-Descent" class="headerlink" title="Vanilla Gradient Descent"></a>Vanilla Gradient Descent</h5><p>$$<br>w^{t+1}  =  w^t - \frac{\eta}{\sqrt{t+1}} \frac{\partial L(u^t)}{\partial W}<br>$$</p>
<p>可以看到 当t越大(t代表求导的次数，也就是输入的训练集的个数)，t+1越大，随着迭代的次数越多<br>$$<br>learningrate = \frac{n}{\sqrt{t+1}}<br>$$<br>learning rate 越小</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Md工具 typora</span><br><span class="line">w^&#123;t+<span class="number">1</span>&#125;  =  w^t - \frac&#123;n&#125;&#123;\sqrt&#123;t+<span class="number">1</span>&#125;&#125; \frac&#123;\partial L(u^t)&#125;&#123;\partial W&#125;</span><br></pre></td></tr></table></figure>
<p><strong>AdagradOptimizer是比较稳定的优化函数，上面的实验也是AdagradOptimizer的准确率最高</strong></p>
<h4 id="使用随机Gradient-Descent-Stochastic-SGD"><a href="#使用随机Gradient-Descent-Stochastic-SGD" class="headerlink" title="使用随机Gradient Descent(Stochastic SGD)"></a>使用随机Gradient Descent(Stochastic SGD)</h4><p>在MNIST数据集中有55000个训练集合。在训练过程中不是每次都选取55000张图片进行训练，而是随机选取图片进行多轮迭代。</p>
<p>这样做的好处是，假如一次训练使损失函数(cross_entropy)到达了斜率为0的点，但不是函数的最低点。函数是局部最低点。这一点的左边的斜率是负值(正值)，右边的斜率是负值(正值)的这一点，优化函数就卡在这个位置，以为这个就是最优解。另一轮的训练又将函数从这一点移开，再进行下一次优化。</p>
<p><img src="/2018/01/08/检验softmax/局部最优解.png" alt="局部最优解"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Size of:</span><br><span class="line">- Training-set:		<span class="number">55000</span></span><br><span class="line">- Test-set:		<span class="number">10000</span></span><br><span class="line">- Validation-set:	<span class="number">5000</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="comment"># tensorflow的官方demo就是使用SGD的方式训练的  </span></span><br><span class="line"><span class="comment"># Train</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">        batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">        print(batch_ys)</span><br><span class="line">        sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;)</span><br></pre></td></tr></table></figure>
<h4 id="控制模型的特征值在相同的范围内，模型能容易收敛"><a href="#控制模型的特征值在相同的范围内，模型能容易收敛" class="headerlink" title="控制模型的特征值在相同的范围内，模型能容易收敛"></a>控制模型的特征值在相同的范围内，模型能容易收敛</h4><p>意思是模型的训练数据要在特定的范围内，一个前一个很大，后一个值很小，训练的次数要更多才能到达最优值。</p>
<p>如果训练的数据集在相对集中的范围内，只需要3次(如右图)就到达了最优解</p>
<p><img src="/2018/01/08/检验softmax/feature_scaling.png" alt="feature_scaling"></p>
<h3 id="得到训练好的权重"><a href="#得到训练好的权重" class="headerlink" title="得到训练好的权重"></a>得到训练好的权重</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">w = np.array([[<span class="number">-1.00487339</span>, - <span class="number">1.15891397</span>, <span class="number">2.16378975</span>],</span><br><span class="line">              [<span class="number">-0.44634497</span>, <span class="number">4.75635386</span>, <span class="number">-4.31000757</span>]])</span><br><span class="line">b = np.array([<span class="number">-0.44633889</span>, <span class="number">4.75634384</span>, <span class="number">-4.31001806</span>])</span><br><span class="line">y = np.matmul([<span class="number">6</span>, <span class="number">1</span>], w) + b </span><br><span class="line"><span class="comment"># [100%][-6.9219242   2.55921388  4.36271287]</span></span><br><span class="line"><span class="comment"># [6,1]的分类是[0,0,1] 看到argmax([-6.9219242   2.55921388  4.36271287]) = 2  因此预测值是正确的</span></span><br><span class="line"><span class="comment"># tensorflow中有一种以arg开头的api，取数组的特定值的index</span></span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/05/MNIST-FOR-ML/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="xiejq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/WechatIMG2.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="虫虫呀">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/05/MNIST-FOR-ML/" itemprop="url">MNIST FOR ML</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-05T16:05:37+08:00">
                2018-01-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="MNIST-FOR-ML"><a href="#MNIST-FOR-ML" class="headerlink" title="MNIST FOR ML"></a>MNIST FOR ML</h2><p>tensorflow<a href="https://www.tensorflow.org/get_started/mnist/beginners" target="_blank" rel="noopener">tensorflow.org</a></p>
<p><a href="https://github.com/ccwhale/tfTutorials/blob/master/tutorials/mnist/mnist_softmax.py" target="_blank" rel="noopener">tensorflow官网源代码</a></p>
<p><a href="https://github.com/ccwhale/tfTutorials/blob/master/tutorials/hvassTutorials/simpleLinearModel.py" target="_blank" rel="noopener">更详细的教程</a></p>
<h3 id="softmax函数"><a href="#softmax函数" class="headerlink" title="softmax函数"></a>softmax函数</h3><h4 id="什么是softmax函数"><a href="#什么是softmax函数" class="headerlink" title="什么是softmax函数"></a>什么是softmax函数</h4><p>对应代码中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</span><br><span class="line"><span class="comment"># tf.matmul(x, W) (-1,784) (784,10)得到y (-1,10)的矩阵</span></span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"><span class="comment"># y是预测值 这是训练定义的模型</span></span><br><span class="line">y = tf.matmul(x, W) + b</span><br></pre></td></tr></table></figure>
<p><img src="/2018/01/05/MNIST-FOR-ML/softmax.jpg" alt="softmax"></p>
<h4 id="交叉熵为什么可以评估系统的好坏"><a href="#交叉熵为什么可以评估系统的好坏" class="headerlink" title="交叉熵为什么可以评估系统的好坏"></a>交叉熵为什么可以评估系统的好坏</h4><p>对应代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string"> 数学知识点2:softmax函数和交叉熵(用交叉熵定义损失函数)</span></span><br><span class="line"><span class="string"> """</span></span><br><span class="line"> <span class="comment">#   tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.nn.softmax(y)),</span></span><br><span class="line"> <span class="comment">#                                 reduction_indices=[1]))</span></span><br><span class="line"> <span class="comment">#</span></span><br><span class="line"> <span class="comment"># can be numerically unstable.</span></span><br><span class="line"> <span class="comment">#</span></span><br><span class="line"> <span class="comment"># So here we use tf.losses.sparse_softmax_cross_entropy on the raw</span></span><br><span class="line"> <span class="comment"># outputs of 'y', and then average across the batch.</span></span><br><span class="line"> cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=y_, logits=y)</span><br></pre></td></tr></table></figure>
<p><a href="https://www.khanacademy.org/computing/computer-science/informationtheory/moderninfotheory/v/information-entropy" target="_blank" rel="noopener">好听的视频</a></p>
<p>交叉熵函数cost的输入是一个个function set。由不同的w,b的模型组成的函数集合。在训练的过程中就是使交叉熵值变小。</p>
<p>对应的代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">数学知识点3:梯度函数</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.8</span>).minimize(cross_entropy)</span><br><span class="line">   <span class="string">"""</span></span><br><span class="line"><span class="string">    调整不同的优化函数显示不同的准确度(控制learning Rate为0.5)</span></span><br><span class="line"><span class="string">    ==============================</span></span><br><span class="line"><span class="string">    | AdamOptimizer      | 0.8679</span></span><br><span class="line"><span class="string">    ==============================</span></span><br><span class="line"><span class="string">    | AdagradOptimizer   | 0.9202</span></span><br><span class="line"><span class="string">    ==============================</span></span><br><span class="line"><span class="string">    | GradientDescentOptimizer | 0.9152</span></span><br><span class="line"><span class="string">    ==============================</span></span><br><span class="line"><span class="string">    调整不同的learning rate(控制梯度函数为GradientDescentOptimizer 0.5=0.9152 )</span></span><br><span class="line"><span class="string">    ==============================</span></span><br><span class="line"><span class="string">    | 0.1                | 0.9097</span></span><br><span class="line"><span class="string">    ==============================</span></span><br><span class="line"><span class="string">    | 0.2                | 0.9163</span></span><br><span class="line"><span class="string">    ==============================</span></span><br><span class="line"><span class="string">    | 0.3                 | 0.9181</span></span><br><span class="line"><span class="string">    ==============================</span></span><br><span class="line"><span class="string">    | 0.8                 | 0.9234</span></span><br><span class="line"><span class="string">    ==============================</span></span><br><span class="line"><span class="string">    """</span></span><br></pre></td></tr></table></figure>
<p>通过对每个参数求偏导的方式找到函数的最低点。</p>
<p>下图在代码中实现的:在w0处求得的偏导，然后根据learning rate(由自己定义的)向右移动得到一个新的较好的w1，每个参数在每次输入中都在改变得到最优的参数。</p>
<p><img src="/2018/01/05/MNIST-FOR-ML/cross_entropy.jpg" alt="cross_entropy"></p>
<h4 id="为什么要用softmax函数"><a href="#为什么要用softmax函数" class="headerlink" title="为什么要用softmax函数"></a>为什么要用softmax函数</h4><ol>
<li><strong>增加一层函数，但是不影响求每个参数偏导的速度</strong></li>
<li><strong>可以将输出值控制到[0,1) 之间，在求损失函数的时候能够保证loss的值始终为正值。</strong></li>
</ol>
<p>第一层 函数 ： y = tf.matmul(x,w)+b</p>
<p>第二次函数 ： s = softmax(y)     0&lt;=s&lt;1=0</p>
<p>第三层函数 :  loss = -y真实值ln s  loss函数的值恒为正值，因此需要取反</p>
<p>利用链式求导法则，求loss对每个w的偏倒数如下。</p>
<p><img src="/2018/01/05/MNIST-FOR-ML/IMG_0712.jpg" alt="IMG_0712"></p>
<p><img src="/2018/01/05/MNIST-FOR-ML/IMG_0713.jpg" alt="IMG_0713"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/04/卷积网络构建函数/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="xiejq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/WechatIMG2.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="虫虫呀">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/04/卷积网络构建函数/" itemprop="url">卷积网络构建函数</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-04T23:31:44+08:00">
                2018-01-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="卷积网络构建函数"><a href="#卷积网络构建函数" class="headerlink" title="卷积网络构建函数"></a>卷积网络构建函数</h2><h3 id="conv2d-tf-nn-conv2d"><a href="#conv2d-tf-nn-conv2d" class="headerlink" title="conv2d = tf.nn.conv2d"></a>conv2d = tf.nn.conv2d</h3><p><a href="">示例代码</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conv2d = tf.nn.conv2d(input_batch,</span><br><span class="line">                          kernel,</span><br><span class="line">                          strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                          padding=<span class="string">'SAME'</span>)</span><br><span class="line"><span class="comment"># padding 有SAME和VALID</span></span><br></pre></td></tr></table></figure>
<p>input_batch: Tensor(“Const:0”, shape=(1, 6, 6, 1), dtype=float32)</p>
<p>kernel: Tensor(“Const_1:0”, shape=(3, 3, 1, 1), dtype=float32)</p>
<p>输入卷积函数的第一个参数必须是四位的</p>
<ol>
<li>image number(输入的图片数目)</li>
<li>y-axis ()</li>
<li>x-axis ()</li>
<li>channels(图片的像素)</li>
</ol>
<p>卷积的维度也必须是四维的</p>
<ol>
<li>filter_size</li>
<li>filter_size</li>
<li>输入的渠道:也就是像素数目</li>
<li>输出的渠道</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">input_batch = tf.constant(</span><br><span class="line">    [</span><br><span class="line">        [</span><br><span class="line">            [[<span class="number">1.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>], [<span class="number">1.0</span>]],</span><br><span class="line">            [[<span class="number">0.0</span>], [<span class="number">1.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>], [<span class="number">1.0</span>], [<span class="number">0.0</span>]],</span><br><span class="line">            [[<span class="number">0.0</span>], [<span class="number">0.0</span>], [<span class="number">1.0</span>], [<span class="number">1.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>]],</span><br><span class="line">            [[<span class="number">1.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>], [<span class="number">1.0</span>], [<span class="number">0.0</span>]],</span><br><span class="line">            [[<span class="number">0.0</span>], [<span class="number">1.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>], [<span class="number">1.0</span>], [<span class="number">0.0</span>]],</span><br><span class="line">            [[<span class="number">0.0</span>], [<span class="number">0.0</span>], [<span class="number">1.0</span>], [<span class="number">0.0</span>], [<span class="number">1.0</span>], [<span class="number">0.0</span>]]</span><br><span class="line">        ]</span><br><span class="line">    ])</span><br><span class="line"><span class="comment"># 第一个方括号有一个方括号(1,)</span></span><br><span class="line"><span class="comment"># 第二个方括号有5个逗号+一个完整的两个括号(1,6)</span></span><br><span class="line"><span class="comment"># 第三个方括号有5个逗号+一个括号(1,6,6)</span></span><br><span class="line"><span class="comment"># 第四个括号有1个括号 (1,6,6,1)</span></span><br><span class="line">kernel = tf.constant([</span><br><span class="line">    [[[<span class="number">1.0</span>]], [[<span class="number">-1.0</span>]], [[<span class="number">-1.0</span>]]],</span><br><span class="line">    [[[<span class="number">-1.0</span>]], [[<span class="number">1.0</span>]], [[<span class="number">-1.0</span>]]],</span><br><span class="line">    [[[<span class="number">-1.0</span>]], [[<span class="number">-1.0</span>]], [[<span class="number">1.0</span>]]]</span><br><span class="line">])</span><br><span class="line"><span class="comment"># 第一个方括号有2个逗号+一个完整的三个括号 (3,)</span></span><br><span class="line"><span class="comment"># 第二个方括号有2个逗号+一个完整的两个括号的(3,3)</span></span><br><span class="line"><span class="comment"># 第三个方括号有1个括号(3,3,1)</span></span><br><span class="line"><span class="comment"># 第四个方括号有一个数字(3,3,1,1)</span></span><br></pre></td></tr></table></figure>
<p>上面构建了一个<code>6*6</code>的输入和一个<code>3*3</code>的内核</p>
<p>如果有两个卷积核</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">两个卷积层同时作用于输入的图片</span></span><br><span class="line"><span class="string">kernel = tf.constant([</span></span><br><span class="line"><span class="string">    [[[1.0, -1.0]], [[-1.0, 1.0]], [[-1.0, -1.0]]],</span></span><br><span class="line"><span class="string">    [[[-1.0, -1.0]], [[1.0, 1.0]], [[-1.0, -1.0]]],</span></span><br><span class="line"><span class="string">    [[[-1.0, -1.0]], [[-1.0, 1.0]], [[1.0, -1.0]]]</span></span><br><span class="line"><span class="string">])</span></span><br><span class="line"><span class="string"># 输出(1,4,4,2)的图片</span></span><br><span class="line"><span class="string">[[[[ 3. -1.]</span></span><br><span class="line"><span class="string">   [-1. -1.]</span></span><br><span class="line"><span class="string">   [-3. -1.]</span></span><br><span class="line"><span class="string">   [-1. -1.]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  [[-3. -1.]</span></span><br><span class="line"><span class="string">   [ 1. -1.]</span></span><br><span class="line"><span class="string">   [ 0. -2.]</span></span><br><span class="line"><span class="string">   [-3.  1.]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  [[-3. -1.]</span></span><br><span class="line"><span class="string">   [-3. -1.]</span></span><br><span class="line"><span class="string">   [ 0. -2.]</span></span><br><span class="line"><span class="string">   [ 1.  1.]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  [[ 3. -1.]</span></span><br><span class="line"><span class="string">   [-2.  0.]</span></span><br><span class="line"><span class="string">   [-2. -4.]</span></span><br><span class="line"><span class="string">   [-1.  3.]]]]</span></span><br><span class="line"><span class="string"># 上例中一个卷积内核有9个参数 一共有3*3*2 = 18个参数</span></span><br><span class="line"><span class="string"># 对(1,4,4,2)的图片用2*2的内核在进行一次卷积输出四维的图片</span></span><br><span class="line"><span class="string"># 一个2*2的内核有2个参数，每个内核要作用上一层输出的二维的立方体，因此一个内核有8个参数</span></span><br><span class="line"><span class="string"># 输出四维的图片，有四个内核，第二层有32个参数</span></span><br></pre></td></tr></table></figure>
<h3 id="在真实场景的应用"><a href="#在真实场景的应用" class="headerlink" title="在真实场景的应用"></a>在真实场景的应用</h3><h4 id="定义一个卷积核"><a href="#定义一个卷积核" class="headerlink" title="定义一个卷积核"></a>定义一个卷积核</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">new_weights</span><span class="params">(shape)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.Variable(tf.truncated_normal(shape, stddev=<span class="number">0.05</span>))</span><br><span class="line"><span class="comment"># 从截断正态分布中随机获取的矩阵(truncate_normal截断正态分布)</span></span><br><span class="line"><span class="comment"># 第一层卷积的shape(5,5,1,16) 包含的参数有400个，这400个参数是由训练得到最佳值</span></span><br><span class="line"><span class="comment"># 第二层卷积的shape(5,5,16,36) 包含5*5*16*36个参数 也是由训练得到</span></span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/12/29/新大陆/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="xiejq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/WechatIMG2.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="虫虫呀">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/12/29/新大陆/" itemprop="url">新大陆</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-12-29T10:37:50+08:00">
                2017-12-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://www.youtube.com/watch?v=fegAeph9UaA" target="_blank" rel="noopener">ML Lecture 1: Regression - Case Study</a></p>
<p><a href="https://www.youtube.com/watch?v=D_S6y0Jm6dQ" target="_blank" rel="noopener">ML Lecture 2: Where does the error come from?</a></p>
<p><a href="https://www.youtube.com/watch?v=yKKNr-QKz2Q" target="_blank" rel="noopener">ML Lecture 3-1: Gradient Descent</a></p>
<p><a href="https://www.youtube.com/watch?v=fZAZUYEeIMg" target="_blank" rel="noopener">ML Lecture 4: Classification</a></p>
<p><a href="https://www.youtube.com/watch?v=hSXFuypLukA" target="_blank" rel="noopener">ML Lecture 5: Logistic Regression</a></p>
<p><a href="https://www.youtube.com/watch?v=Dr-WRlEFefw" target="_blank" rel="noopener">ML Lecture 6: Brief Introduction of Deep Learning</a></p>
<p><a href="https://www.youtube.com/watch?v=ibJpTrp5mcE" target="_blank" rel="noopener">ML Lecture 7: Backpropagation</a></p>
<p><a href="https://www.youtube.com/watch?v=Lx3l4lOrquw" target="_blank" rel="noopener">ML Lecture 8-1: “Hello world” of deep learning</a></p>
<p><a href="https://www.youtube.com/watch?v=5BJDJd-dzzg" target="_blank" rel="noopener">ML Lecture 8-2: Keras 2.0</a></p>
<p><a href="https://www.youtube.com/watch?v=L8unuZNpWw8" target="_blank" rel="noopener">ML Lecture 8-3: Keras Demo</a></p>
<p><a href="https://www.youtube.com/watch?v=FrKWiRv254g" target="_blank" rel="noopener">ML Lecture 10: Convolutional Neural Network</a></p>
<p><a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials" target="_blank" rel="noopener">Hvass-Labs tensorflow教程</a></p>
<p><a href="http://blog.topspeedsnail.com/archives/10399" target="_blank" rel="noopener">超级有意思的博客</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/12/28/MNIST-简单线性模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="xiejq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/WechatIMG2.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="虫虫呀">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/12/28/MNIST-简单线性模型/" itemprop="url">MNIST_简单线性模型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-12-28T23:13:05+08:00">
                2017-12-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="MNIST数据集"><a href="#MNIST数据集" class="headerlink" title="MNIST数据集"></a>MNIST数据集</h2><h3 id="打印MNIST数据集的长度"><a href="#打印MNIST数据集的长度" class="headerlink" title="打印MNIST数据集的长度"></a>打印MNIST数据集的长度</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Size of:</span><br><span class="line">- Training-set:		<span class="number">55000</span></span><br><span class="line">- Test-set:		<span class="number">10000</span></span><br><span class="line">- Validation-set:	<span class="number">5000</span></span><br></pre></td></tr></table></figure>
<h3 id="MNIST的one-hot编码"><a href="#MNIST的one-hot编码" class="headerlink" title="MNIST的one-hot编码"></a>MNIST的one-hot编码</h3><p>向量为1的下标是图片对应的分类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">print(data.test.labels[<span class="number">0</span>:<span class="number">5</span>, :])</span><br><span class="line"><span class="comment"># 将one-hot编码转化为对应的数字</span></span><br><span class="line">data.test.cls = np.array([label.argmax() <span class="keyword">for</span> label <span class="keyword">in</span> data.test.labels])</span><br><span class="line">print(data.test.cls[<span class="number">0</span>:<span class="number">5</span>])</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensorflow的arg开头的api表示的是取向量的index</span></span><br><span class="line"><span class="string">tensorflow的以reduce开头的api表示的是将矩阵降维(压扁)</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">[[ <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">1.</span>  <span class="number">0.</span>  <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">1.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">0.</span>  <span class="number">1.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">1.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">1.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>]]</span><br><span class="line">[<span class="number">7</span> <span class="number">2</span> <span class="number">1</span> <span class="number">0</span> <span class="number">4</span>]</span><br></pre></td></tr></table></figure>
<h3 id="定义辅助函数画出MNIST的图像"><a href="#定义辅助函数画出MNIST的图像" class="headerlink" title="定义辅助函数画出MNIST的图像"></a>定义辅助函数画出MNIST的图像</h3><p><a href="https://github.com/ccwhale/tfTutorials/blob/master/tutorials/hvassTutorials/helperFunction.py" target="_blank" rel="noopener">辅助函数</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot the images and labels using our helper-function above.</span></span><br><span class="line"><span class="comment"># Get the first images from the test-set.</span></span><br><span class="line"><span class="comment"># 画出数据集的前9个图片</span></span><br><span class="line">images = data.test.images[<span class="number">0</span>:<span class="number">9</span>]</span><br><span class="line">helperFunction.plot_images(images=images, cls_true=cls_true)</span><br></pre></td></tr></table></figure>
<h2 id="三个步骤"><a href="#三个步骤" class="headerlink" title="三个步骤"></a>三个步骤</h2><h3 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create the model</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, img_size_flat])</span><br><span class="line">y_true = tf.placeholder(tf.float32, [<span class="keyword">None</span>, num_classes])</span><br><span class="line">y_true_cls = tf.placeholder(tf.int64, [<span class="keyword">None</span>])</span><br><span class="line">weights = tf.Variable(tf.zeros([img_size_flat, num_classes]))</span><br><span class="line">biases = tf.Variable(tf.zeros([num_classes]))</span><br><span class="line"></span><br><span class="line">logits = tf.matmul(x, weights) + biases <span class="comment">#定义模型</span></span><br><span class="line">y_pred = tf.nn.softmax(logits) <span class="comment"># 预测值</span></span><br><span class="line">y_pred_cls = tf.argmax(y_pred, axis=<span class="number">1</span>) <span class="comment"># 按照列reduce 返回每行为1的下标值</span></span><br></pre></td></tr></table></figure>
<p>x: 表示有 -1 * 784维的输入 -1表示输入任意的图片数</p>
<p>W: 权重，10表示是要将图片分类为0,1,2..9的分类数目</p>
<p>b:表示偏差</p>
<p>通过这个模型得到预测值为 y = tf.argmax(y, 1) 每行最大的那个数的下标就是预测得到的分类值。</p>
<p>假设有两张一个2*2像素的图片，这两张图片可能是表示的是1或者2</p>
<p>[[1,2,3,4]</p>
<p>[3,4,5,6]]</p>
<p>定义[[w1,w2],[w3,w4],[w5,w6],[w7,w8]]</p>
<p>矩阵相乘得到第一行: <code>[1*w1+2*w2+3*w3+4*w4,1*w2+2*w4+3*w6+4*w8]</code></p>
<p>实际告诉我们第一张图片是1，那就是调整所有的w和偏差使得1<em>w2+2</em>w4+3<em>w6+4</em>w8较大，1<em>w1+2</em>w2+3<em>w3+4</em>w4较小。每次训练都是在更新所有的w和b</p>
<h3 id="tensorflow的三个概念"><a href="#tensorflow的三个概念" class="headerlink" title="tensorflow的三个概念"></a>tensorflow的三个概念</h3><h4 id="tensorflow图的概念-谷歌翻译"><a href="#tensorflow图的概念-谷歌翻译" class="headerlink" title="tensorflow图的概念(谷歌翻译)"></a>tensorflow图的概念(谷歌翻译)</h4><blockquote>
<p>TensorFlow的全部目的就是要有一个所谓的计算图，比直接在Python中执行相同的计算更有效。 TensorFlow可以比NumPy更高效，因为TensorFlow知道必须执行的整个计算图，而NumPy一次只知道单个数学运算的计算。</p>
<p>TensorFlow还可以自动计算优化图的变量所需的梯度，以使模型更好地运行。这是因为图形是简单数学表达式的组合，所以整个图形的梯度可以使用导数的链式法则来计算。</p>
<p>TensorFlow也可以利用多核CPU和GPU，而Google甚至为TensorFlow建立了专门的芯片，这些芯片被称为TPU（张量处理单元），甚至比GPU更快。</p>
<p>张量流图由以下部分组成：</p>
<p>用于将输入更改为图形的占位符变量。</p>
<p>要进行优化的模型变量，以使模型更好地运行。</p>
<p>这个模型本质上只是一个数学函数，它在占位符变量和模型变量的输入中计算一些输出。可以用来指导变量优化的成本度量。更新模型变量的优化方法。</p>
<p>另外，张量流图还可以包含各种调试语句，例如用于记录使用TensorBoard的数据，本教程未涉及。</p>
</blockquote>
<p> <strong>注解</strong>:</p>
<ol>
<li>tensorflow的整个图形梯度可以使用导数(partial derivatives)的链式法则来计算,这里要写总结</li>
<li>tensorflow可以在GPU上加速，要设置批次</li>
</ol>
<h5 id="占位变量"><a href="#占位变量" class="headerlink" title="占位变量"></a>占位变量</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We know that MNIST images are 28 pixels in each dimension.</span></span><br><span class="line">img_size = <span class="number">28</span></span><br><span class="line"><span class="comment"># Images are stored in one-dimensional arrays of this length.</span></span><br><span class="line">img_size_flat = img_size * img_size</span><br><span class="line"><span class="comment"># Tuple with height and width of images used to reshape arrays.</span></span><br><span class="line">img_shape = (img_size, img_size)</span><br><span class="line"><span class="comment"># Number of classes, one class for each of 10 digits.</span></span><br><span class="line">num_classes = <span class="number">10</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, img_size_flat])</span><br><span class="line">y_true = tf.placeholder(tf.float32, [<span class="keyword">None</span>, num_classes])</span><br><span class="line">y_true_cls = tf.placeholder(tf.int64, [<span class="keyword">None</span>])</span><br></pre></td></tr></table></figure>
<h5 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">weights = tf.Variable(tf.zeros([img_size_flat, num_classes]))</span><br><span class="line">biases = tf.Variable(tf.zeros([num_classes]))</span><br></pre></td></tr></table></figure>
<h3 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><p>真实值和预测值之间的差距就是损失函数，在训练过程中每次更新variable都将损失值减小，就是训练的目的。</p>
<ol>
<li><a href="">为什么交叉熵可以衡量真实值和预测值的差距</a></li>
<li><a href="">什么是softmax</a></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,</span><br><span class="line">                                                       labels=y_true)</span><br><span class="line">cost = tf.reduce_mean(cross_entropy)</span><br></pre></td></tr></table></figure>
<h3 id="定义优化函数"><a href="#定义优化函数" class="headerlink" title="定义优化函数"></a>定义优化函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(cost)</span><br></pre></td></tr></table></figure>
<p><a href="">优化函数的原理,tensorflow包含的优化API,梯度函数的学习率如何影响机器学习,什么是向前传播向后传播,为什么要使用softmax</a></p>
<h3 id="开始训练"><a href="#开始训练" class="headerlink" title="开始训练"></a>开始训练</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train</span></span><br><span class="line">  <span class="comment"># 训练循环1000次</span></span><br><span class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment"># 每次batch从60000个training data中随机选取100个图片进行训练，一次epoch更新100次变量的数据</span></span><br><span class="line">    batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Test trained model</span></span><br><span class="line">  correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), y_)</span><br><span class="line">  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">  print(sess.run(W))</span><br><span class="line">  print(sess.run(b))     </span><br><span class="line">  print(sess.run(accuracy, feed_dict=&#123;x: mnist.test.images,</span><br><span class="line">                                      y_: mnist.test.labels&#125;))</span><br></pre></td></tr></table></figure>
<p>使用小的随机数据叫做随机训练，在这个例子中，叫做随机梯度下降。理想状态下，我们希望每次训练所有的数据都能用上，因为这能给我们所做的训练一个好的场景，但是这样的训练开销很大。</p>
<h2 id="辅助函数显示性能"><a href="#辅助函数显示性能" class="headerlink" title="辅助函数显示性能"></a>辅助函数显示性能</h2><p><a href="https://github.com/ccwhale/tfTutorials/tree/master/tutorials/hvassTutorials/performance" target="_blank" rel="noopener">看看性能</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/WechatIMG2.jpeg"
                alt="xiejq" />
            
              <p class="site-author-name" itemprop="name">xiejq</p>
              <p class="site-description motion-element" itemprop="description">入门AI</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">13</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://github.com/ccwhale" target="_blank" title="GitHub">
                    
                      <i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:392629711@qq.com" target="_blank" title="E-Mail">
                    
                      <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
            
          </div>

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">xiejq</span>

  
</div>









        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
