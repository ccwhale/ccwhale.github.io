<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/cc.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/cc.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/cc.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="虫虫呀">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="虫虫呀">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="虫虫呀">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '谢江琼'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>虫虫呀</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">虫虫呀</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">苦练含笑半步癫</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/08/cifar数据集/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="xiejq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/WechatIMG2.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="虫虫呀">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/08/cifar数据集/" itemprop="url">cifar数据集</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-08T17:08:05+08:00">
                2018-01-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="cifar数据集官方demo"><a href="#cifar数据集官方demo" class="headerlink" title="cifar数据集官方demo"></a>cifar数据集官方demo</h2><p><img src="/2018/01/08/cifar数据集/professional_demo.png" alt="professional_demo"></p>
<h3 id="数据集的结构"><a href="#数据集的结构" class="headerlink" title="数据集的结构"></a>数据集的结构</h3><h4 id="数据集下载和解压"><a href="#数据集下载和解压" class="headerlink" title="数据集下载和解压"></a>数据集下载和解压</h4><p>源码是cifar.py</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/08/检验softmax/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="xiejq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/WechatIMG2.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="虫虫呀">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/08/检验softmax/" itemprop="url">检验softmax</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-08T09:05:47+08:00">
                2018-01-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="是否大于5岁的测试demo"><a href="#是否大于5岁的测试demo" class="headerlink" title="是否大于5岁的测试demo"></a>是否大于5岁的测试demo</h2><p><a href="http://geek.csdn.net/news/detail/163735" target="_blank" rel="noopener">csdn</a></p>
<p><a href="https://github.com/ccwhale/tensorflowDemo/blob/master/trials/validateSoftmax.py" target="_blank" rel="noopener">可执行代码的位置</a></p>
<p><img src="/2018/01/08/检验softmax/firstpicture.png" alt="firstpicture"></p>
<h3 id="生成训练数据和测试数据"><a href="#生成训练数据和测试数据" class="headerlink" title="生成训练数据和测试数据"></a>生成训练数据和测试数据</h3><p>模型的label数据集是one-hot编码或者是它的真实值</p>
<p>如果year大于5岁，则标签设置为：[0, 0, 1]；  数组的第二个位置为1，因此标签的真实分类是2</p>
<p>否则，标签设置为：[0, 1, 0]。数组的第一个位置为1，因此标签的真实分类是1</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成(6000,2)的训练集合</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'label.txt'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> file:</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'train.txt'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">6000</span>):</span><br><span class="line">            a = random.randint(<span class="number">0</span>, <span class="number">9</span>)</span><br><span class="line">            <span class="keyword">if</span> a &gt; <span class="number">5</span>:</span><br><span class="line">                file.write(<span class="string">'2'</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                file.write(<span class="string">'1'</span>)</span><br><span class="line">            f.write(str(a) + <span class="string">'1'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成(1000,2)的训练集合</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'label_test.txt'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> file:</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'train_test.txt'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">            a = random.randint(<span class="number">0</span>, <span class="number">9</span>)</span><br><span class="line">            <span class="keyword">if</span> a &gt; <span class="number">5</span>:</span><br><span class="line">                file.write(<span class="string">'2'</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                file.write(<span class="string">'1'</span>)</span><br><span class="line">            f.write(str(a) + <span class="string">'1'</span>)</span><br></pre></td></tr></table></figure>
<h2 id="定义训练模型"><a href="#定义训练模型" class="headerlink" title="定义训练模型"></a>定义训练模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义训练模型 tf.placeholder是是需要喂养的数据 tf.Variable()是需要通过训练不断优化得到的变量</span></span><br><span class="line"><span class="comment"># y_是[None,2]训练数据的真实分类</span></span><br><span class="line">    x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">2</span>])</span><br><span class="line">    w = tf.Variable(tf.zeros([<span class="number">2</span>, <span class="number">3</span>]))</span><br><span class="line">    b = tf.Variable(tf.zeros([<span class="number">3</span>]))</span><br><span class="line">    y = tf.matmul(x, w) + b</span><br><span class="line">    y_ = tf.placeholder(tf.int64, [<span class="keyword">None</span>])  <span class="comment"># 一维的,就是它的真实值</span></span><br></pre></td></tr></table></figure>
<p><img src="/2018/01/08/检验softmax/linear_model.jpg" alt="linear_model"></p>
<p>一次输入就会更新定义的6个w和3个b(bias偏差)</p>
<h2 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cross_entropy = tf.losses.sparse_softmax_cross_entropy(logits=y, labels=y_)</span><br></pre></td></tr></table></figure>
<p>logits=y 是预测值</p>
<p>labels=y_ 是真实值</p>
<p><a href="https://ccwhale.github.io/2017/12/28/tensorflowAPI-and-Mathematics/" target="_blank" rel="noopener">以reduce开头的api将二维矩阵压扁</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这个函数是由一系列操作封装好的</span></span><br><span class="line"><span class="comment"># The raw formulation of cross-entropy,</span></span><br><span class="line"><span class="comment">#  python有一组以reduce开头的api,作用是给将二维矩阵压扁。0代表行1代表列，</span></span><br><span class="line"><span class="comment">#  因此下面代码是按列将数组压扁，按照列，就是从列看过去，从左边看过去，压缩二维数组，也就是取每行的和</span></span><br><span class="line"><span class="comment">#  如果没有指定行或者列，取所有值的平均值</span></span><br><span class="line"><span class="comment">#   tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.nn.softmax(y)), # 矩阵的dot product</span></span><br><span class="line"><span class="comment">#                                        reduction_indices=[1]))</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># can be numerically unstable.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># So here we use tf.losses.sparse_softmax_cross_entropy on the raw</span></span><br><span class="line"><span class="comment"># logit outputs of 'y', and then average across the batch.</span></span><br></pre></td></tr></table></figure>
<h2 id="定义优化函数"><a href="#定义优化函数" class="headerlink" title="定义优化函数"></a>定义优化函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(cross_entropy)</span><br></pre></td></tr></table></figure>
<p>tensorflow中包含很多优化函数。优化函数有两个参数:</p>
<ol>
<li>learning rate 学习率，决定每次输入一次数据，根据偏导更新每个权重的步长</li>
<li>cross_entropy 交叉熵函数，学习的过程就是使函数的熵值变小。</li>
</ol>
<p><strong>优化函数的输入也是函数</strong></p>
<p><a href="https://ccwhale.github.io/2018/01/05/MNIST-FOR-ML/" target="_blank" rel="noopener">交叉熵为什么可以评价权重的好坏</a></p>
<p>0.5是优化函数的learning rate。</p>
<h3 id="梯度下降-Gradient-Descent-的策略"><a href="#梯度下降-Gradient-Descent-的策略" class="headerlink" title="梯度下降(Gradient Descent)的策略"></a>梯度下降(Gradient Descent)的策略</h3><h4 id="使用可以自我调整learning-rate的优化函数"><a href="#使用可以自我调整learning-rate的优化函数" class="headerlink" title="使用可以自我调整learning rate的优化函数"></a>使用可以自我调整learning rate的优化函数</h4><p>在训练的过程中，我们假定刚开始训练的时候，离cross_entropy的最小值(最低点)很远，这个时候我们希望learning rate很大。当训练快要接近函数的最低点的时候，我们希望learning rate很小。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.8</span>).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    调整不同的优化函数显示不同的准确度(控制learning Rate为0.5)</span></span><br><span class="line"><span class="string">    ==============================</span></span><br><span class="line"><span class="string">    | AdamOptimizer      | 0.8679</span></span><br><span class="line"><span class="string">    ==============================</span></span><br><span class="line"><span class="string">    | AdagradOptimizer   | 0.9202</span></span><br><span class="line"><span class="string">    ==============================</span></span><br><span class="line"><span class="string">    | GradientDescentOptimizer | 0.9152</span></span><br><span class="line"><span class="string">    ==============================</span></span><br><span class="line"><span class="string">    调整不同的learning rate(控制梯度函数为GradientDescentOptimizer 0.5=0.9152 )</span></span><br><span class="line"><span class="string">    ==============================</span></span><br><span class="line"><span class="string">    | 0.1                | 0.9097</span></span><br><span class="line"><span class="string">    ==============================</span></span><br><span class="line"><span class="string">    | 0.2                | 0.9163</span></span><br><span class="line"><span class="string">    ==============================</span></span><br><span class="line"><span class="string">    | 0.3                 | 0.9181</span></span><br><span class="line"><span class="string">    ==============================</span></span><br><span class="line"><span class="string">    | 0.8                 | 0.9234</span></span><br><span class="line"><span class="string">    ==============================</span></span><br></pre></td></tr></table></figure>
<h5 id="AdagradOptimizer"><a href="#AdagradOptimizer" class="headerlink" title="AdagradOptimizer"></a>AdagradOptimizer</h5><p>是一个自我调整learning rate的函数</p>
<p>$$w^{t+1} = w^t - \frac{\eta}{\sum_{i=0}^t (g^i)^2} * g^t$$<br>优化函数认为训练的开始梯度Gradient(也就是偏导)很大，离训练的终点很小，需要很大的learning rate</p>
<p>在AdagradOptimizer方法中：</p>
<p>$g^t$很大的时候，分母也很大，$\eta$就很小</p>
<p>$g^t$很小的时候，分母不一定很小，因为分母是前面所有导数的和，$\eta$就可能还是很小</p>
<p><strong>AdagradOptimizer</strong> 的作用是强化反差：</p>
<p>当$g^t$之前都很小，突然变得很大，$\eta$就变的很小，此时移动的步长就很小了(不知道理解的对不对，待修正)</p>
<h5 id="Vanilla-Gradient-Descent"><a href="#Vanilla-Gradient-Descent" class="headerlink" title="Vanilla Gradient Descent"></a>Vanilla Gradient Descent</h5><p>$$<br>w^{t+1}  =  w^t - \frac{\eta}{\sqrt{t+1}} \frac{\partial L(u^t)}{\partial W}<br>$$</p>
<p>可以看到 当t越大(t代表求导的次数，也就是输入的训练集的个数)，t+1越大，随着迭代的次数越多<br>$$<br>learningrate = \frac{n}{\sqrt{t+1}}<br>$$<br>learning rate 越小</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Md工具 typora</span><br><span class="line">w^&#123;t+<span class="number">1</span>&#125;  =  w^t - \frac&#123;n&#125;&#123;\sqrt&#123;t+<span class="number">1</span>&#125;&#125; \frac&#123;\partial L(u^t)&#125;&#123;\partial W&#125;</span><br></pre></td></tr></table></figure>
<p><strong>AdagradOptimizer是比较稳定的优化函数，上面的实验也是AdagradOptimizer的准确率最高</strong></p>
<h4 id="使用随机Gradient-Descent-Stochastic-SGD"><a href="#使用随机Gradient-Descent-Stochastic-SGD" class="headerlink" title="使用随机Gradient Descent(Stochastic SGD)"></a>使用随机Gradient Descent(Stochastic SGD)</h4><p>在MNIST数据集中有55000个训练集合。在训练过程中不是每次都选取55000张图片进行训练，而是随机选取图片进行多轮迭代。</p>
<p>这样做的好处是，假如一次训练使损失函数(cross_entropy)到达了斜率为0的点，但不是函数的最低点。函数是局部最低点。这一点的左边的斜率是负值(正值)，右边的斜率是负值(正值)的这一点，优化函数就卡在这个位置，以为这个就是最优解。另一轮的训练又将函数从这一点移开，再进行下一次优化。</p>
<p><img src="/2018/01/08/检验softmax/局部最优解.png" alt="局部最优解"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Size of:</span><br><span class="line">- Training-set:		<span class="number">55000</span></span><br><span class="line">- Test-set:		<span class="number">10000</span></span><br><span class="line">- Validation-set:	<span class="number">5000</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="comment"># tensorflow的官方demo就是使用SGD的方式训练的  </span></span><br><span class="line"><span class="comment"># Train</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">        batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">        print(batch_ys)</span><br><span class="line">        sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;)</span><br></pre></td></tr></table></figure>
<h4 id="控制模型的特征值在相同的范围内，模型能容易收敛"><a href="#控制模型的特征值在相同的范围内，模型能容易收敛" class="headerlink" title="控制模型的特征值在相同的范围内，模型能容易收敛"></a>控制模型的特征值在相同的范围内，模型能容易收敛</h4><p>意思是模型的训练数据要在特定的范围内，一个前一个很大，后一个值很小，训练的次数要更多才能到达最优值。</p>
<p>如果训练的数据集在相对集中的范围内，只需要3次(如右图)就到达了最优解</p>
<p><img src="/2018/01/08/检验softmax/feature_scaling.png" alt="feature_scaling"></p>
<h3 id="得到训练好的权重"><a href="#得到训练好的权重" class="headerlink" title="得到训练好的权重"></a>得到训练好的权重</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">w = np.array([[<span class="number">-1.00487339</span>, - <span class="number">1.15891397</span>, <span class="number">2.16378975</span>],</span><br><span class="line">              [<span class="number">-0.44634497</span>, <span class="number">4.75635386</span>, <span class="number">-4.31000757</span>]])</span><br><span class="line">b = np.array([<span class="number">-0.44633889</span>, <span class="number">4.75634384</span>, <span class="number">-4.31001806</span>])</span><br><span class="line">y = np.matmul([<span class="number">6</span>, <span class="number">1</span>], w) + b </span><br><span class="line"><span class="comment"># [100%][-6.9219242   2.55921388  4.36271287]</span></span><br><span class="line"><span class="comment"># [6,1]的分类是[0,0,1] 看到argmax([-6.9219242   2.55921388  4.36271287]) = 2  因此预测值是正确的</span></span><br><span class="line"><span class="comment"># tensorflow中有一种以arg开头的api，取数组的特定值的index</span></span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/05/MNIST-FOR-ML/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="xiejq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/WechatIMG2.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="虫虫呀">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/05/MNIST-FOR-ML/" itemprop="url">MNIST FOR ML</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-05T16:05:37+08:00">
                2018-01-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="MNIST-FOR-ML"><a href="#MNIST-FOR-ML" class="headerlink" title="MNIST FOR ML"></a>MNIST FOR ML</h2><p>tensorflow<a href="https://www.tensorflow.org/get_started/mnist/beginners" target="_blank" rel="noopener">tensorflow.org</a></p>
<p><a href="https://github.com/ccwhale/tfTutorials/blob/master/tutorials/mnist/mnist_softmax.py" target="_blank" rel="noopener">tensorflow官网源代码</a></p>
<p><a href="https://github.com/ccwhale/tfTutorials/blob/master/tutorials/hvassTutorials/simpleLinearModel.py" target="_blank" rel="noopener">更详细的教程</a></p>
<h3 id="softmax函数"><a href="#softmax函数" class="headerlink" title="softmax函数"></a>softmax函数</h3><h4 id="什么是softmax函数"><a href="#什么是softmax函数" class="headerlink" title="什么是softmax函数"></a>什么是softmax函数</h4><p>对应代码中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</span><br><span class="line"><span class="comment"># tf.matmul(x, W) (-1,784) (784,10)得到y (-1,10)的矩阵</span></span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"><span class="comment"># y是预测值 这是训练定义的模型</span></span><br><span class="line">y = tf.matmul(x, W) + b</span><br></pre></td></tr></table></figure>
<p><img src="/2018/01/05/MNIST-FOR-ML/softmax.jpg" alt="softmax"></p>
<h4 id="交叉熵为什么可以评估系统的好坏"><a href="#交叉熵为什么可以评估系统的好坏" class="headerlink" title="交叉熵为什么可以评估系统的好坏"></a>交叉熵为什么可以评估系统的好坏</h4><p>对应代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string"> 数学知识点2:softmax函数和交叉熵(用交叉熵定义损失函数)</span></span><br><span class="line"><span class="string"> """</span></span><br><span class="line"> <span class="comment">#   tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.nn.softmax(y)),</span></span><br><span class="line"> <span class="comment">#                                 reduction_indices=[1]))</span></span><br><span class="line"> <span class="comment">#</span></span><br><span class="line"> <span class="comment"># can be numerically unstable.</span></span><br><span class="line"> <span class="comment">#</span></span><br><span class="line"> <span class="comment"># So here we use tf.losses.sparse_softmax_cross_entropy on the raw</span></span><br><span class="line"> <span class="comment"># outputs of 'y', and then average across the batch.</span></span><br><span class="line"> cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=y_, logits=y)</span><br></pre></td></tr></table></figure>
<p><a href="https://www.khanacademy.org/computing/computer-science/informationtheory/moderninfotheory/v/information-entropy" target="_blank" rel="noopener">好听的视频</a></p>
<p>交叉熵函数cost的输入是一个个function set。由不同的w,b的模型组成的函数集合。在训练的过程中就是使交叉熵值变小。</p>
<p>对应的代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">数学知识点3:梯度函数</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.8</span>).minimize(cross_entropy)</span><br><span class="line">   <span class="string">"""</span></span><br><span class="line"><span class="string">    调整不同的优化函数显示不同的准确度(控制learning Rate为0.5)</span></span><br><span class="line"><span class="string">    ==============================</span></span><br><span class="line"><span class="string">    | AdamOptimizer      | 0.8679</span></span><br><span class="line"><span class="string">    ==============================</span></span><br><span class="line"><span class="string">    | AdagradOptimizer   | 0.9202</span></span><br><span class="line"><span class="string">    ==============================</span></span><br><span class="line"><span class="string">    | GradientDescentOptimizer | 0.9152</span></span><br><span class="line"><span class="string">    ==============================</span></span><br><span class="line"><span class="string">    调整不同的learning rate(控制梯度函数为GradientDescentOptimizer 0.5=0.9152 )</span></span><br><span class="line"><span class="string">    ==============================</span></span><br><span class="line"><span class="string">    | 0.1                | 0.9097</span></span><br><span class="line"><span class="string">    ==============================</span></span><br><span class="line"><span class="string">    | 0.2                | 0.9163</span></span><br><span class="line"><span class="string">    ==============================</span></span><br><span class="line"><span class="string">    | 0.3                 | 0.9181</span></span><br><span class="line"><span class="string">    ==============================</span></span><br><span class="line"><span class="string">    | 0.8                 | 0.9234</span></span><br><span class="line"><span class="string">    ==============================</span></span><br><span class="line"><span class="string">    """</span></span><br></pre></td></tr></table></figure>
<p>通过对每个参数求偏导的方式找到函数的最低点。</p>
<p>下图在代码中实现的:在w0处求得的偏导，然后根据learning rate(由自己定义的)向右移动得到一个新的较好的w1，每个参数在每次输入中都在改变得到最优的参数。</p>
<p><img src="/2018/01/05/MNIST-FOR-ML/cross_entropy.jpg" alt="cross_entropy"></p>
<h4 id="为什么要用softmax函数"><a href="#为什么要用softmax函数" class="headerlink" title="为什么要用softmax函数"></a>为什么要用softmax函数</h4><ol>
<li><strong>增加一层函数，但是不影响求每个参数偏导的速度</strong></li>
<li><strong>可以将输出值控制到[0,1) 之间，在求损失函数的时候能够保证loss的值始终为正值。</strong></li>
</ol>
<p>第一层 函数 ： y = tf.matmul(x,w)+b</p>
<p>第二次函数 ： s = softmax(y)     0&lt;=s&lt;1=0</p>
<p>第三层函数 :  loss = -y真实值ln s  loss函数的值恒为正值，因此需要取反</p>
<p>利用链式求导法则，求loss对每个w的偏倒数如下。</p>
<p><img src="/2018/01/05/MNIST-FOR-ML/IMG_0712.jpg" alt="IMG_0712"></p>
<p><img src="/2018/01/05/MNIST-FOR-ML/IMG_0713.jpg" alt="IMG_0713"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/04/卷积网络构建函数/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="xiejq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/WechatIMG2.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="虫虫呀">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/04/卷积网络构建函数/" itemprop="url">卷积网络构建函数</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-04T23:31:44+08:00">
                2018-01-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="卷积网络构建函数"><a href="#卷积网络构建函数" class="headerlink" title="卷积网络构建函数"></a>卷积网络构建函数</h2><h3 id="conv2d-tf-nn-conv2d"><a href="#conv2d-tf-nn-conv2d" class="headerlink" title="conv2d = tf.nn.conv2d"></a>conv2d = tf.nn.conv2d</h3><p><a href="">示例代码</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conv2d = tf.nn.conv2d(input_batch,</span><br><span class="line">                          kernel,</span><br><span class="line">                          strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                          padding=<span class="string">'SAME'</span>)</span><br><span class="line"><span class="comment"># padding 有SAME和VALID</span></span><br></pre></td></tr></table></figure>
<p>input_batch: Tensor(“Const:0”, shape=(1, 6, 6, 1), dtype=float32)</p>
<p>kernel: Tensor(“Const_1:0”, shape=(3, 3, 1, 1), dtype=float32)</p>
<p>输入卷积函数的第一个参数必须是四位的</p>
<ol>
<li>image number(输入的图片数目)</li>
<li>y-axis ()</li>
<li>x-axis ()</li>
<li>channels(图片的像素)</li>
</ol>
<p>卷积的维度也必须是四维的</p>
<ol>
<li>filter_size</li>
<li>filter_size</li>
<li>输入的渠道:也就是像素数目</li>
<li>输出的渠道</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">input_batch = tf.constant(</span><br><span class="line">    [</span><br><span class="line">        [</span><br><span class="line">            [[<span class="number">1.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>], [<span class="number">1.0</span>]],</span><br><span class="line">            [[<span class="number">0.0</span>], [<span class="number">1.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>], [<span class="number">1.0</span>], [<span class="number">0.0</span>]],</span><br><span class="line">            [[<span class="number">0.0</span>], [<span class="number">0.0</span>], [<span class="number">1.0</span>], [<span class="number">1.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>]],</span><br><span class="line">            [[<span class="number">1.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>], [<span class="number">1.0</span>], [<span class="number">0.0</span>]],</span><br><span class="line">            [[<span class="number">0.0</span>], [<span class="number">1.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>], [<span class="number">1.0</span>], [<span class="number">0.0</span>]],</span><br><span class="line">            [[<span class="number">0.0</span>], [<span class="number">0.0</span>], [<span class="number">1.0</span>], [<span class="number">0.0</span>], [<span class="number">1.0</span>], [<span class="number">0.0</span>]]</span><br><span class="line">        ]</span><br><span class="line">    ])</span><br><span class="line"><span class="comment"># 第一个方括号有一个方括号(1,)</span></span><br><span class="line"><span class="comment"># 第二个方括号有5个逗号+一个完整的两个括号(1,6)</span></span><br><span class="line"><span class="comment"># 第三个方括号有5个逗号+一个括号(1,6,6)</span></span><br><span class="line"><span class="comment"># 第四个括号有1个括号 (1,6,6,1)</span></span><br><span class="line">kernel = tf.constant([</span><br><span class="line">    [[[<span class="number">1.0</span>]], [[<span class="number">-1.0</span>]], [[<span class="number">-1.0</span>]]],</span><br><span class="line">    [[[<span class="number">-1.0</span>]], [[<span class="number">1.0</span>]], [[<span class="number">-1.0</span>]]],</span><br><span class="line">    [[[<span class="number">-1.0</span>]], [[<span class="number">-1.0</span>]], [[<span class="number">1.0</span>]]]</span><br><span class="line">])</span><br><span class="line"><span class="comment"># 第一个方括号有2个逗号+一个完整的三个括号 (3,)</span></span><br><span class="line"><span class="comment"># 第二个方括号有2个逗号+一个完整的两个括号的(3,3)</span></span><br><span class="line"><span class="comment"># 第三个方括号有1个括号(3,3,1)</span></span><br><span class="line"><span class="comment"># 第四个方括号有一个数字(3,3,1,1)</span></span><br></pre></td></tr></table></figure>
<p>上面构建了一个<code>6*6</code>的输入和一个<code>3*3</code>的内核</p>
<p>如果有两个卷积核</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">两个卷积层同时作用于输入的图片</span></span><br><span class="line"><span class="string">kernel = tf.constant([</span></span><br><span class="line"><span class="string">    [[[1.0, -1.0]], [[-1.0, 1.0]], [[-1.0, -1.0]]],</span></span><br><span class="line"><span class="string">    [[[-1.0, -1.0]], [[1.0, 1.0]], [[-1.0, -1.0]]],</span></span><br><span class="line"><span class="string">    [[[-1.0, -1.0]], [[-1.0, 1.0]], [[1.0, -1.0]]]</span></span><br><span class="line"><span class="string">])</span></span><br><span class="line"><span class="string"># 输出(1,4,4,2)的图片</span></span><br><span class="line"><span class="string">[[[[ 3. -1.]</span></span><br><span class="line"><span class="string">   [-1. -1.]</span></span><br><span class="line"><span class="string">   [-3. -1.]</span></span><br><span class="line"><span class="string">   [-1. -1.]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  [[-3. -1.]</span></span><br><span class="line"><span class="string">   [ 1. -1.]</span></span><br><span class="line"><span class="string">   [ 0. -2.]</span></span><br><span class="line"><span class="string">   [-3.  1.]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  [[-3. -1.]</span></span><br><span class="line"><span class="string">   [-3. -1.]</span></span><br><span class="line"><span class="string">   [ 0. -2.]</span></span><br><span class="line"><span class="string">   [ 1.  1.]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  [[ 3. -1.]</span></span><br><span class="line"><span class="string">   [-2.  0.]</span></span><br><span class="line"><span class="string">   [-2. -4.]</span></span><br><span class="line"><span class="string">   [-1.  3.]]]]</span></span><br><span class="line"><span class="string"># 上例中一个卷积内核有9个参数 一共有3*3*2 = 18个参数</span></span><br><span class="line"><span class="string"># 对(1,4,4,2)的图片用2*2的内核在进行一次卷积输出四维的图片</span></span><br><span class="line"><span class="string"># 一个2*2的内核有2个参数，每个内核要作用上一层输出的二维的立方体，因此一个内核有8个参数</span></span><br><span class="line"><span class="string"># 输出四维的图片，有四个内核，第二层有32个参数</span></span><br></pre></td></tr></table></figure>
<h3 id="在真实场景的应用"><a href="#在真实场景的应用" class="headerlink" title="在真实场景的应用"></a>在真实场景的应用</h3><h4 id="定义一个卷积核"><a href="#定义一个卷积核" class="headerlink" title="定义一个卷积核"></a>定义一个卷积核</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">new_weights</span><span class="params">(shape)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.Variable(tf.truncated_normal(shape, stddev=<span class="number">0.05</span>))</span><br><span class="line"><span class="comment"># 从截断正态分布中随机获取的矩阵(truncate_normal截断正态分布)</span></span><br><span class="line"><span class="comment"># 第一层卷积的shape(5,5,1,16) 包含的参数有400个，这400个参数是由训练得到最佳值</span></span><br><span class="line"><span class="comment"># 第二层卷积的shape(5,5,16,36) 包含5*5*16*36个参数 也是由训练得到</span></span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/12/29/新大陆/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="xiejq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/WechatIMG2.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="虫虫呀">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/12/29/新大陆/" itemprop="url">新大陆</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-12-29T10:37:50+08:00">
                2017-12-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://www.youtube.com/watch?v=fegAeph9UaA" target="_blank" rel="noopener">ML Lecture 1: Regression - Case Study</a></p>
<p><a href="https://www.youtube.com/watch?v=D_S6y0Jm6dQ" target="_blank" rel="noopener">ML Lecture 2: Where does the error come from?</a></p>
<p><a href="https://www.youtube.com/watch?v=yKKNr-QKz2Q" target="_blank" rel="noopener">ML Lecture 3-1: Gradient Descent</a></p>
<p><a href="https://www.youtube.com/watch?v=fZAZUYEeIMg" target="_blank" rel="noopener">ML Lecture 4: Classification</a></p>
<p><a href="https://www.youtube.com/watch?v=hSXFuypLukA" target="_blank" rel="noopener">ML Lecture 5: Logistic Regression</a></p>
<p><a href="https://www.youtube.com/watch?v=Dr-WRlEFefw" target="_blank" rel="noopener">ML Lecture 6: Brief Introduction of Deep Learning</a></p>
<p><a href="https://www.youtube.com/watch?v=ibJpTrp5mcE" target="_blank" rel="noopener">ML Lecture 7: Backpropagation</a></p>
<p><a href="https://www.youtube.com/watch?v=Lx3l4lOrquw" target="_blank" rel="noopener">ML Lecture 8-1: “Hello world” of deep learning</a></p>
<p><a href="https://www.youtube.com/watch?v=5BJDJd-dzzg" target="_blank" rel="noopener">ML Lecture 8-2: Keras 2.0</a></p>
<p><a href="https://www.youtube.com/watch?v=L8unuZNpWw8" target="_blank" rel="noopener">ML Lecture 8-3: Keras Demo</a></p>
<p><a href="https://www.youtube.com/watch?v=FrKWiRv254g" target="_blank" rel="noopener">ML Lecture 10: Convolutional Neural Network</a></p>
<p><a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials" target="_blank" rel="noopener">Hvass-Labs tensorflow教程</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/12/28/MNIST-简单线性模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="xiejq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/WechatIMG2.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="虫虫呀">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/12/28/MNIST-简单线性模型/" itemprop="url">MNIST_简单线性模型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-12-28T23:13:05+08:00">
                2017-12-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="MNIST数据集"><a href="#MNIST数据集" class="headerlink" title="MNIST数据集"></a>MNIST数据集</h2><h3 id="打印MNIST数据集的长度"><a href="#打印MNIST数据集的长度" class="headerlink" title="打印MNIST数据集的长度"></a>打印MNIST数据集的长度</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Size of:</span><br><span class="line">- Training-set:		<span class="number">55000</span></span><br><span class="line">- Test-set:		<span class="number">10000</span></span><br><span class="line">- Validation-set:	<span class="number">5000</span></span><br></pre></td></tr></table></figure>
<h3 id="MNIST的one-hot编码"><a href="#MNIST的one-hot编码" class="headerlink" title="MNIST的one-hot编码"></a>MNIST的one-hot编码</h3><p>向量为1的下标是图片对应的分类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">print(data.test.labels[<span class="number">0</span>:<span class="number">5</span>, :])</span><br><span class="line"><span class="comment"># 将one-hot编码转化为对应的数字</span></span><br><span class="line">data.test.cls = np.array([label.argmax() <span class="keyword">for</span> label <span class="keyword">in</span> data.test.labels])</span><br><span class="line">print(data.test.cls[<span class="number">0</span>:<span class="number">5</span>])</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensorflow的arg开头的api表示的是取向量的index</span></span><br><span class="line"><span class="string">tensorflow的以reduce开头的api表示的是将矩阵降维(压扁)</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">[[ <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">1.</span>  <span class="number">0.</span>  <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">1.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">0.</span>  <span class="number">1.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">1.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">1.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>]]</span><br><span class="line">[<span class="number">7</span> <span class="number">2</span> <span class="number">1</span> <span class="number">0</span> <span class="number">4</span>]</span><br></pre></td></tr></table></figure>
<h3 id="定义辅助函数画出MNIST的图像"><a href="#定义辅助函数画出MNIST的图像" class="headerlink" title="定义辅助函数画出MNIST的图像"></a>定义辅助函数画出MNIST的图像</h3><p><a href="https://github.com/ccwhale/tfTutorials/blob/master/tutorials/hvassTutorials/helperFunction.py" target="_blank" rel="noopener">辅助函数</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot the images and labels using our helper-function above.</span></span><br><span class="line"><span class="comment"># Get the first images from the test-set.</span></span><br><span class="line"><span class="comment"># 画出数据集的前9个图片</span></span><br><span class="line">images = data.test.images[<span class="number">0</span>:<span class="number">9</span>]</span><br><span class="line">helperFunction.plot_images(images=images, cls_true=cls_true)</span><br></pre></td></tr></table></figure>
<h2 id="三个步骤"><a href="#三个步骤" class="headerlink" title="三个步骤"></a>三个步骤</h2><h3 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create the model</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, img_size_flat])</span><br><span class="line">y_true = tf.placeholder(tf.float32, [<span class="keyword">None</span>, num_classes])</span><br><span class="line">y_true_cls = tf.placeholder(tf.int64, [<span class="keyword">None</span>])</span><br><span class="line">weights = tf.Variable(tf.zeros([img_size_flat, num_classes]))</span><br><span class="line">biases = tf.Variable(tf.zeros([num_classes]))</span><br><span class="line"></span><br><span class="line">logits = tf.matmul(x, weights) + biases <span class="comment">#定义模型</span></span><br><span class="line">y_pred = tf.nn.softmax(logits) <span class="comment"># 预测值</span></span><br><span class="line">y_pred_cls = tf.argmax(y_pred, axis=<span class="number">1</span>) <span class="comment"># 按照列reduce 返回每行为1的下标值</span></span><br></pre></td></tr></table></figure>
<p>x: 表示有 -1 * 784维的输入 -1表示输入任意的图片数</p>
<p>W: 权重，10表示是要将图片分类为0,1,2..9的分类数目</p>
<p>b:表示偏差</p>
<p>通过这个模型得到预测值为 y = tf.argmax(y, 1) 每行最大的那个数的下标就是预测得到的分类值。</p>
<p>假设有两张一个2*2像素的图片，这两张图片可能是表示的是1或者2</p>
<p>[[1,2,3,4]</p>
<p>[3,4,5,6]]</p>
<p>定义[[w1,w2],[w3,w4],[w5,w6],[w7,w8]]</p>
<p>矩阵相乘得到第一行: <code>[1*w1+2*w2+3*w3+4*w4,1*w2+2*w4+3*w6+4*w8]</code></p>
<p>实际告诉我们第一张图片是1，那就是调整所有的w和偏差使得1<em>w2+2</em>w4+3<em>w6+4</em>w8较大，1<em>w1+2</em>w2+3<em>w3+4</em>w4较小。每次训练都是在更新所有的w和b</p>
<h3 id="tensorflow的三个概念"><a href="#tensorflow的三个概念" class="headerlink" title="tensorflow的三个概念"></a>tensorflow的三个概念</h3><h4 id="tensorflow图的概念-谷歌翻译"><a href="#tensorflow图的概念-谷歌翻译" class="headerlink" title="tensorflow图的概念(谷歌翻译)"></a>tensorflow图的概念(谷歌翻译)</h4><blockquote>
<p>TensorFlow的全部目的就是要有一个所谓的计算图，比直接在Python中执行相同的计算更有效。 TensorFlow可以比NumPy更高效，因为TensorFlow知道必须执行的整个计算图，而NumPy一次只知道单个数学运算的计算。</p>
<p>TensorFlow还可以自动计算优化图的变量所需的梯度，以使模型更好地运行。这是因为图形是简单数学表达式的组合，所以整个图形的梯度可以使用导数的链式法则来计算。</p>
<p>TensorFlow也可以利用多核CPU和GPU，而Google甚至为TensorFlow建立了专门的芯片，这些芯片被称为TPU（张量处理单元），甚至比GPU更快。</p>
<p>张量流图由以下部分组成：</p>
<p>用于将输入更改为图形的占位符变量。</p>
<p>要进行优化的模型变量，以使模型更好地运行。</p>
<p>这个模型本质上只是一个数学函数，它在占位符变量和模型变量的输入中计算一些输出。可以用来指导变量优化的成本度量。更新模型变量的优化方法。</p>
<p>另外，张量流图还可以包含各种调试语句，例如用于记录使用TensorBoard的数据，本教程未涉及。</p>
</blockquote>
<p> <strong>注解</strong>:</p>
<ol>
<li>tensorflow的整个图形梯度可以使用导数(partial derivatives)的链式法则来计算,这里要写总结</li>
<li>tensorflow可以在GPU上加速，要设置批次</li>
</ol>
<h5 id="占位变量"><a href="#占位变量" class="headerlink" title="占位变量"></a>占位变量</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We know that MNIST images are 28 pixels in each dimension.</span></span><br><span class="line">img_size = <span class="number">28</span></span><br><span class="line"><span class="comment"># Images are stored in one-dimensional arrays of this length.</span></span><br><span class="line">img_size_flat = img_size * img_size</span><br><span class="line"><span class="comment"># Tuple with height and width of images used to reshape arrays.</span></span><br><span class="line">img_shape = (img_size, img_size)</span><br><span class="line"><span class="comment"># Number of classes, one class for each of 10 digits.</span></span><br><span class="line">num_classes = <span class="number">10</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, img_size_flat])</span><br><span class="line">y_true = tf.placeholder(tf.float32, [<span class="keyword">None</span>, num_classes])</span><br><span class="line">y_true_cls = tf.placeholder(tf.int64, [<span class="keyword">None</span>])</span><br></pre></td></tr></table></figure>
<h5 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">weights = tf.Variable(tf.zeros([img_size_flat, num_classes]))</span><br><span class="line">biases = tf.Variable(tf.zeros([num_classes]))</span><br></pre></td></tr></table></figure>
<h3 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><p>真实值和预测值之间的差距就是损失函数，在训练过程中每次更新variable都将损失值减小，就是训练的目的。</p>
<ol>
<li><a href="">为什么交叉熵可以衡量真实值和预测值的差距</a></li>
<li><a href="">什么是softmax</a></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,</span><br><span class="line">                                                       labels=y_true)</span><br><span class="line">cost = tf.reduce_mean(cross_entropy)</span><br></pre></td></tr></table></figure>
<h3 id="定义优化函数"><a href="#定义优化函数" class="headerlink" title="定义优化函数"></a>定义优化函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(cost)</span><br></pre></td></tr></table></figure>
<p><a href="">优化函数的原理,tensorflow包含的优化API,梯度函数的学习率如何影响机器学习,什么是向前传播向后传播,为什么要使用softmax</a></p>
<h3 id="开始训练"><a href="#开始训练" class="headerlink" title="开始训练"></a>开始训练</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train</span></span><br><span class="line">  <span class="comment"># 训练循环1000次</span></span><br><span class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment"># 每次batch从60000个training data中随机选取100个图片进行训练，一次epoch更新100次变量的数据</span></span><br><span class="line">    batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Test trained model</span></span><br><span class="line">  correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), y_)</span><br><span class="line">  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">  print(sess.run(W))</span><br><span class="line">  print(sess.run(b))     </span><br><span class="line">  print(sess.run(accuracy, feed_dict=&#123;x: mnist.test.images,</span><br><span class="line">                                      y_: mnist.test.labels&#125;))</span><br></pre></td></tr></table></figure>
<p>使用小的随机数据叫做随机训练，在这个例子中，叫做随机梯度下降。理想状态下，我们希望每次训练所有的数据都能用上，因为这能给我们所做的训练一个好的场景，但是这样的训练开销很大。</p>
<h2 id="辅助函数显示性能"><a href="#辅助函数显示性能" class="headerlink" title="辅助函数显示性能"></a>辅助函数显示性能</h2><p><a href="https://github.com/ccwhale/tfTutorials/tree/master/tutorials/hvassTutorials/performance" target="_blank" rel="noopener">看看性能</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/12/28/tensorflowAPI-and-Mathematics/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="xiejq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/WechatIMG2.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="虫虫呀">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/12/28/tensorflowAPI-and-Mathematics/" itemprop="url">tensorflowAPI and Mathematics</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-12-28T09:42:28+08:00">
                2017-12-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="tensorflowAPI"><a href="#tensorflowAPI" class="headerlink" title="tensorflowAPI"></a>tensorflowAPI</h2><h3 id="矩阵的点乘-dot-product"><a href="#矩阵的点乘-dot-product" class="headerlink" title="矩阵的点乘(dot product)"></a>矩阵的点乘(dot product)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a = tf.constant([[<span class="number">-1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">	print(sess.run(a * a))</span><br><span class="line"><span class="comment"># [[  1.   4.   9.  16.]</span></span><br><span class="line"><span class="comment">#  [  1.   4.   9.  16.]]</span></span><br></pre></td></tr></table></figure>
<h3 id="数与矩阵相乘"><a href="#数与矩阵相乘" class="headerlink" title="数与矩阵相乘"></a>数与矩阵相乘</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(sess.run(<span class="number">2</span>*a))</span><br><span class="line"><span class="comment">#[[-2.  4.  6.  8.]</span></span><br><span class="line"><span class="comment"># [ 2.  4.  6.  8.]]</span></span><br></pre></td></tr></table></figure>
<h3 id="矩阵与矩阵相乘"><a href="#矩阵与矩阵相乘" class="headerlink" title="矩阵与矩阵相乘"></a>矩阵与矩阵相乘</h3><p><strong>定义</strong>:设<strong>A</strong>=(aij)是一个m x s矩阵，<strong>B</strong>=(bij)是一个s x n矩阵，那么规定矩阵<strong>A</strong>与矩阵<strong>B</strong>的的成绩是一个m x n矩阵<strong>C</strong>=(cij).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([[<span class="number">-1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]]) <span class="comment"># 2*4</span></span><br><span class="line">b = tf.constant([[<span class="number">-1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>], [<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]) <span class="comment"># 4*2 </span></span><br><span class="line">y = tf.matmul(a, b) + c <span class="comment"># 定义y的表达式 2*2</span></span><br><span class="line">c = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>])</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(y))</span><br><span class="line"><span class="comment"># [[ 23.  30.]</span></span><br><span class="line"><span class="comment">#  [ 21.  34.]]</span></span><br><span class="line"><span class="comment"># [1.0, 2.0] 每一行的对应位置加了c</span></span><br><span class="line"><span class="comment"># c = tf.constant([1.0])</span></span><br><span class="line"><span class="comment"># 每个元素加了1.0 如果c的维度大于2会报错</span></span><br></pre></td></tr></table></figure>
<h3 id="tf-argmax-x-x-x-x-axis"><a href="#tf-argmax-x-x-x-x-axis" class="headerlink" title="tf.argmax([x,x,x,x],axis)"></a>tf.argmax([x,x,x,x],axis)</h3><p>返回指定维度张量最大值的下标</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">d = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>, <span class="number">9.0</span>]])</span><br><span class="line"><span class="comment">#Tensor("Const_3:0", shape=(2, 4), dtype=float32)</span></span><br><span class="line"><span class="comment">#[[ 1.  2.  3.  4.]</span></span><br><span class="line"><span class="comment">#  [ 6.  7.  8.  9.]]</span></span><br><span class="line">print(sess.run(tf.argmax(d, <span class="number">0</span>))) <span class="comment"># 0表示第一维度 对应 列 矩阵是2*4的矩阵,第一个数是行,第二个数是列</span></span><br><span class="line">print(sess.run(tf.argmax(d, <span class="number">1</span>))) <span class="comment"># 1表示第二维度 对应 行 tf的参数都是0表示列，1表示行。死记</span></span><br><span class="line"><span class="comment"># [1 1 1 1]</span></span><br><span class="line"><span class="comment"># [3 3]</span></span><br></pre></td></tr></table></figure>
<h3 id="实现softmax"><a href="#实现softmax" class="headerlink" title="实现softmax"></a>实现softmax</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">y = tf.matmul(a, b) + c</span><br><span class="line">s = tf.nn.softmax(y)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">     print(sess.run(s))</span><br><span class="line"><span class="comment"># [[  9.11051233e-04   9.99089003e-01]  一行相加等于1</span></span><br><span class="line"><span class="comment">#  [  2.26032421e-06   9.99997735e-01]] 一行相加等于1</span></span><br><span class="line">print(sess.run(tf.reduce_sum(s, <span class="number">0</span>))) <span class="comment"># 按列相加</span></span><br><span class="line">print(sess.run(tf.reduce_sum(s, <span class="number">1</span>))) <span class="comment"># 按行相加</span></span><br><span class="line"><span class="comment"># [  9.13311553e-04   1.99908674e+00]</span></span><br><span class="line"><span class="comment"># [ 1.  1.]</span></span><br></pre></td></tr></table></figure>
<h3 id="实现平均数"><a href="#实现平均数" class="headerlink" title="实现平均数"></a>实现平均数</h3><p>axis=0 表示按列，axis = 1 表示按行，也可以这么理解：<br>axis=0 按第0个维度reduce，reduce就是压扁，</p>
<p>[[-1.  2.  3.  4.]<br> [ 1.  2.  3.  4.]<br> [ 9.  8.  7.  6.]] </p>
<p>按第0个维度，按行压扁，从上向上看，取平均数，也就是取每列的平均数</p>
<p>按第1个维度，按列压扁，从左往右看，去平均数，也就是取每行的平均数</p>
<p>keep_dims =True 看过去咋样就咋样，不会把取到的值转置为1</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">m = tf.constant([[<span class="number">-1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">9</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="comment">#[[-1.  2.  3.  4.]</span></span><br><span class="line"><span class="comment">#  [ 1.  2.  3.  4.]</span></span><br><span class="line"><span class="comment">#  [ 9.  8.  7.  6.]]</span></span><br><span class="line">print(sess.run(tf.reduce_mean(m, <span class="number">0</span>))) <span class="comment"># 按列求平均数</span></span><br><span class="line"><span class="comment"># [ 3.          4.          4.33333349  4.66666651]</span></span><br><span class="line">print(sess.run(tf.reduce_mean(m, <span class="number">0</span>, <span class="keyword">True</span>)))</span><br><span class="line"><span class="comment"># [[ 3.          4.          4.33333349  4.66666651]]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">除非keep_dims是真，tensor的rank都会将为1，如果keep_dims是真，被降低的维度被保留为1</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">print(sess.run(tf.reduce_mean(m, <span class="number">1</span>, <span class="keyword">True</span>)))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">[[ 2. ]</span></span><br><span class="line"><span class="string"> [ 2.5]</span></span><br><span class="line"><span class="string"> [ 7.5]]</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/12/24/机器学习相关概念/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="xiejq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/WechatIMG2.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="虫虫呀">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/12/24/机器学习相关概念/" itemprop="url">机器学习相关概念</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-12-24T21:37:57+08:00">
                2017-12-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><h3 id="特征-属性-标签"><a href="#特征-属性-标签" class="headerlink" title="特征 属性 标签"></a>特征 属性 标签</h3><p>要让机器认识世界，首先要有数据。要判断草莓甜不甜，就要先收集一些关于草莓的数据。</p>
<p>比如个头较小，色泽鲜艳、质地柔软的草莓是甜的；</p>
<p>个头较大，色泽叫浅，质地较硬的草莓是酸的</p>
<p>在机器学习中，大小，色泽和质地被称为特征(Feature)，较小，较大，鲜艳，较软较硬是对应的属性值(attribute value)，酸和甜被称为标签(label)</p>
<p>机器学习就是通过学习特征和标签的关系，判断草莓是不是甜的。</p>
<p>通过数据学的模型的过程就是学习(learning)也称为训练(training)</p>
<h3 id="过拟合-泛化"><a href="#过拟合-泛化" class="headerlink" title="过拟合 泛化"></a>过拟合 泛化</h3><p><strong>过拟合：在学习的过程中，太过认真地认识已有的草莓，会造成无法判断其他草莓甜不甜的状况，叫做过拟合(overfitting)</strong></p>
<p><strong>泛化：我们希望学习得到的模型能很好地认识新的草莓，这种能力被称为泛化(generalization)</strong></p>
<p><strong>一般来说，训练样本越多，模型的泛化能力越好就越能判断新草莓是不是甜的</strong></p>
<p><strong>模型就是一个函数，通过大规模的数据得到函数的参数的值—自己的理解</strong></p>
<h2 id="三大学习"><a href="#三大学习" class="headerlink" title="三大学习"></a>三大学习</h2><h3 id="监督学习-概念学习"><a href="#监督学习-概念学习" class="headerlink" title="监督学习 概念学习"></a>监督学习 概念学习</h3><blockquote>
<p>从前，小明很喜欢吃苹果，为了让他认识苹果，妈妈找来一个大苹果，一个小苹果，一个青苹果，告诉小明这些都是苹果，最后，妈妈告诉小明[不管大小、颜色如何变化，圆圆的，有把儿的就是苹果]现在小明知道什么是苹果了。</p>
</blockquote>
<p>在人类和动物感知中，这通常被称为<strong>概念学习</strong></p>
<p>在人工智能领域，这个过程被称为<strong>监督学习</strong></p>
<p>在上面草莓的例子中，知道特征和标签，训练得出模型的过程是一种监督学习的过程</p>
<h3 id="无监督学习-归纳推理"><a href="#无监督学习-归纳推理" class="headerlink" title="无监督学习 归纳推理"></a>无监督学习 归纳推理</h3><blockquote>
<p>长大后的小明不仅认识了苹果还是番茄，小皮球，小明觉得这些东西有相似之处，妈妈告诉他，这种相似之处是红色</p>
</blockquote>
<p>在人类行为中，我们将这个过程称为<strong>归纳推理</strong></p>
<p>在人工智能的世界，我们叫它<strong>无监督学习</strong></p>
<h3 id="增强学习"><a href="#增强学习" class="headerlink" title="增强学习"></a>增强学习</h3><blockquote>
<p>进入初中后的小明，面临这考试，作为一名优秀的学生，每一次考试结束，小明都会认真观察批改好的试卷，将其中做错的题目单独找出重新解答，直到得出正确的答案。</p>
</blockquote>
<p>人类用思考和修正错误的方式，提升自己对知识的掌握和认知。</p>
<p>在人工智能领域，我们将这个过程称为<strong>增强学习</strong></p>
<h2 id="强人工智能和弱人工智能"><a href="#强人工智能和弱人工智能" class="headerlink" title="强人工智能和弱人工智能"></a>强人工智能和弱人工智能</h2><h3 id="强人工智能"><a href="#强人工智能" class="headerlink" title="强人工智能"></a>强人工智能</h3><p>强人工智能也称为通用人工智能(Artificial General Intelligence AGI):</p>
<p>具备与人类同等智慧、或超越人类智慧的机器。</p>
<h4 id="如何判定一个机器拥有强人工智能"><a href="#如何判定一个机器拥有强人工智能" class="headerlink" title="如何判定一个机器拥有强人工智能"></a>如何判定一个机器拥有强人工智能</h4><p>咖啡测试，机器人学生测试，图灵测试</p>
<p>图灵测试：Turing Test 人类不能判定与之对话的机器还是人类，就认定机器通过了图灵测试。</p>
<h3 id="弱功能智能"><a href="#弱功能智能" class="headerlink" title="弱功能智能"></a>弱功能智能</h3><p>弱人工智能不具备人类一样的完整的认知能力。只需要完成特定的任务(Applied Intelligence)</p>
<p>也称为应用人工智能。</p>
<h4 id="弱人工智能分为四个领域："><a href="#弱人工智能分为四个领域：" class="headerlink" title="弱人工智能分为四个领域："></a>弱人工智能分为四个领域：</h4><ol>
<li>计算机视觉：让计算机看见世界</li>
<li>语音识别：将语音转换为文字</li>
<li>自然语言处理：让计算机理解人类语言</li>
<li>推荐/专家系统：帮助机器实现个性化推荐系统、解决特定问题 的专家系统</li>
</ol>
<h2 id="弱人工智能的四大领域"><a href="#弱人工智能的四大领域" class="headerlink" title="弱人工智能的四大领域"></a>弱人工智能的四大领域</h2><h3 id="计算机视觉"><a href="#计算机视觉" class="headerlink" title="计算机视觉"></a>计算机视觉</h3><p>计算机视觉是一门研究让机器’看’见世界的科学。人类能够认出照片中的食物，推荐它们之间的关系，但对于计算机来说，图像只是一连串RGB数值，没有任何意义。</p>
<h4 id="计算机视觉的核心问题"><a href="#计算机视觉的核心问题" class="headerlink" title="计算机视觉的核心问题:"></a>计算机视觉的核心问题:</h4><p>让计算机看见事件有四个阶段，也是计算机视觉的四个核心问题：</p>
<ol>
<li>图像分类(image classification),为不同的图片打上对应标签</li>
<li>物体检测(Object Recognition),找到物理的位置，并认出他们是什么</li>
<li>语义分割(Semantic Segmentation),找到物体之间的关系和联系</li>
<li>视频分析(Video Analysis),看懂视频中的内容</li>
</ol>
<p>前两者让计算机看见世界，后两者让计算机看懂世界。</p>
<h4 id="计算机视觉的用处"><a href="#计算机视觉的用处" class="headerlink" title="计算机视觉的用处"></a>计算机视觉的用处</h4><p>手机拍照的人脸定位，银行里的人证比对，大到自动驾驶，医疗影像诊断。</p>
<p>最近为了媷羊毛下了个51信用卡管家，添加信用卡用的是拍照自动识别卡号，想到一个app都会用到这些技术了，感觉自己落后了。</p>
<h3 id="语音识别"><a href="#语音识别" class="headerlink" title="语音识别"></a>语音识别</h3><p>与上面的不同，语音识别有且只有一个任务：将人类的语音转化为文字</p>
<p>机器转化的方式:将一段语音的声波按帧切开，用帧组成状态，用状态组成因素，再将因素合成单词，语音就变成了文字。</p>
<p>与语音相关，属于人工智能的研究范围内的任务包含:</p>
<ol>
<li>声纹识别(Speaker Recogintion),即识别说话这是谁</li>
<li>语音合成(Speech Synthesis),即将文字信息转化为人类所听得懂的语音</li>
</ol>
<h3 id="自然语言处理"><a href="#自然语言处理" class="headerlink" title="自然语言处理"></a>自然语言处理</h3><p>用机器处理和运用人类的语言。</p>
<p>自然语言处理要设计语言理解的层面，语言的复杂是的这个模块的任务也很复杂，比如：</p>
<ol>
<li>机器翻译系统(Machine Translation):谷歌翻译，有道翻译</li>
<li>问答系统(Question Answering)：图灵测试就是典型的问答系统，能够自动回答问题的对话系统</li>
<li>中文自动分词(Chinese Word Segmentation)，使用计算机自动对中文文本进行词语切分。</li>
</ol>
<p>信息抽取，阅读理解，自动摘要，文本分类 etc</p>
<h3 id="推荐系统"><a href="#推荐系统" class="headerlink" title="推荐系统"></a>推荐系统</h3><p>信息过滤系统.</p>
<p>基于内容的推荐方法(Content-based Filtering):根据物品的属性为它们打上标签，再通过这些标签计算它们之间的相似度，当在搜索栏中键入关键字，机器就能为你推荐你想要的以及与它相似的物品。</p>
<p>协同过滤(Collaborative Filtering)：一旦我们已经购买过了某种物品，我们就不需要更多类似的东西。还有很多东西我们的确需要，只是没有表达出来。协同过滤就是通过数据找到与你相似的用户，通过他们的行为和他们喜欢的内容，为你推荐你可能感兴趣的物品或内容。</p>
<p>以上来源微信公众号KnowingAI知智</p>
<p>### </p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/12/24/搭建tensorflow开发环境/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="xiejq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/WechatIMG2.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="虫虫呀">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/12/24/搭建tensorflow开发环境/" itemprop="url">搭建tensorflow开发环境</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-12-24T14:08:17+08:00">
                2017-12-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="搭建开发环境"><a href="#搭建开发环境" class="headerlink" title="搭建开发环境"></a>搭建开发环境</h2><h3 id="安装tensorflow-1-4-1"><a href="#安装tensorflow-1-4-1" class="headerlink" title="安装tensorflow 1.4.1"></a>安装tensorflow 1.4.1</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 默认安装到python3.6</span></span><br><span class="line">pip3 install tensorflow </span><br><span class="line"><span class="comment"># 本机存在python3.5 3.6 可以更细分pip命令，确保pip的版本在8以上</span></span><br><span class="line">pip3 --version</span><br><span class="line"><span class="comment">#9.0.1 from /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (python 3.6)</span></span><br><span class="line">pip3.5 install tensorflow</span><br><span class="line"><span class="comment"># 指定安装tensorflow的版本</span></span><br><span class="line">pip3 install tensorflow==1.1.0</span><br><span class="line"><span class="comment"># 查看python中按照tensorflow的版本</span></span><br><span class="line">python <span class="comment"># 进入python解释器</span></span><br><span class="line">import tensorflow as tf</span><br><span class="line">tf.__version__</span><br><span class="line"><span class="comment"># 输出tensorflow的版本</span></span><br></pre></td></tr></table></figure>
<p><a href="https://www.tensorflow.org/install/install_mac" target="_blank" rel="noopener">tenserflow安装</a></p>
<h3 id="更新tensorflow的版本"><a href="#更新tensorflow的版本" class="headerlink" title="更新tensorflow的版本"></a>更新tensorflow的版本</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pip3 install --upgrade tensorflow </span><br><span class="line"><span class="comment"># RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6 #14273</span></span><br><span class="line"><span class="comment"># 根据本机python的版本和操作系统的版本进行更新</span></span><br><span class="line">pip3 install --ignore-installed --upgrade <span class="string">"https://github.com/lakshayg/tensorflow-build/raw/master/tensorflow-1.4.1-cp36-cp36m-macosx_10_13_x86_64.whl"</span></span><br></pre></td></tr></table></figure>
<p><a href="https://github.com/tensorflow/tensorflow/issues/14273" target="_blank" rel="noopener">更新tensorflow出现问题</a></p>
<p><a href="https://github.com/lakshayg/tensorflow-build" target="_blank" rel="noopener">支持AVX，FMA，SSE的TensorFlow二进制文件</a></p>
<h3 id="安装PyCharm"><a href="#安装PyCharm" class="headerlink" title="安装PyCharm"></a>安装PyCharm</h3><p><a href="https://www.jetbrains.com/pycharm/download/#section=mac" target="_blank" rel="noopener">下载地址</a></p>
<p><a href="http://www.imsxm.com/jetbrains-license-server.html" target="_blank" rel="noopener">激活</a></p>
<p>安装pycharm相应的插件(Jetbrains的markdown插件and so on)</p>
<h2 id="熟悉tensorflow"><a href="#熟悉tensorflow" class="headerlink" title="熟悉tensorflow"></a>熟悉tensorflow</h2><h3 id="models"><a href="#models" class="headerlink" title="models"></a>models</h3><p><strong>仅支持tensorflow1.4以上的版本，才能成功运行</strong></p>
<p><a href="https://github.com/tensorflow/models" target="_blank" rel="noopener">fetch工程</a></p>
<h3 id="tensorflow的examples"><a href="#tensorflow的examples" class="headerlink" title="tensorflow的examples"></a>tensorflow的examples</h3><p>tensorflow中自己包含深度学习的例子。fetch工程后，必须把example模块单独拿出来放到新的模块才能运行。</p>
<p><a href="https://github.com/tensorflow" target="_blank" rel="noopener">tensorflow搭建</a></p>
<p>以上，tensorflow的环境搭建完成。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/WechatIMG2.jpeg"
                alt="xiejq" />
            
              <p class="site-author-name" itemprop="name">xiejq</p>
              <p class="site-description motion-element" itemprop="description">入门AI</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://github.com/ccwhale" target="_blank" title="GitHub">
                    
                      <i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:392629711@qq.com" target="_blank" title="E-Mail">
                    
                      <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
            
          </div>

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">xiejq</span>

  
</div>









        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
